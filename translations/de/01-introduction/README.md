<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c3e07ca58d0b8a3f47d3bf5728541e0a",
  "translation_date": "2025-12-13T13:03:03+00:00",
  "source_file": "01-introduction/README.md",
  "language_code": "de"
}
-->
# Modul 01: Einstieg mit LangChain4j

## Inhaltsverzeichnis

- [Was Sie lernen werden](../../../01-introduction)
- [Voraussetzungen](../../../01-introduction)
- [Das Kernproblem verstehen](../../../01-introduction)
- [Tokens verstehen](../../../01-introduction)
- [Wie Speicher funktioniert](../../../01-introduction)
- [Wie dies LangChain4j verwendet](../../../01-introduction)
- [Azure OpenAI Infrastruktur bereitstellen](../../../01-introduction)
- [Die Anwendung lokal ausf√ºhren](../../../01-introduction)
- [Die Anwendung verwenden](../../../01-introduction)
  - [Zustandsloser Chat (linkes Panel)](../../../01-introduction)
  - [Zustandsbehafteter Chat (rechtes Panel)](../../../01-introduction)
- [N√§chste Schritte](../../../01-introduction)

## Was Sie lernen werden

Wenn Sie den Schnellstart abgeschlossen haben, haben Sie gesehen, wie man Eingabeaufforderungen sendet und Antworten erh√§lt. Das ist die Grundlage, aber echte Anwendungen ben√∂tigen mehr. Dieses Modul zeigt Ihnen, wie Sie konversationelle KI bauen, die sich an den Kontext erinnert und den Zustand beibeh√§lt ‚Äì der Unterschied zwischen einer einmaligen Demo und einer produktionsreifen Anwendung.

Wir verwenden im gesamten Leitfaden Azure OpenAI's GPT-5, da seine fortgeschrittenen F√§higkeiten im logischen Denken das Verhalten verschiedener Muster deutlicher machen. Wenn Sie Speicher hinzuf√ºgen, sehen Sie den Unterschied klar. Das erleichtert das Verst√§ndnis, was jede Komponente Ihrer Anwendung bringt.

Sie bauen eine Anwendung, die beide Muster demonstriert:

**Zustandsloser Chat** ‚Äì Jede Anfrage ist unabh√§ngig. Das Modell hat kein Ged√§chtnis an vorherige Nachrichten. Dies ist das Muster, das Sie im Schnellstart verwendet haben.

**Zustandsbehaftete Konversation** ‚Äì Jede Anfrage enth√§lt den Gespr√§chsverlauf. Das Modell beh√§lt den Kontext √ºber mehrere Gespr√§chsrunden hinweg. Das ist, was Produktionsanwendungen ben√∂tigen.

## Voraussetzungen

- Azure-Abonnement mit Zugriff auf Azure OpenAI
- Java 21, Maven 3.9+
- Azure CLI (https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)
- Azure Developer CLI (azd) (https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd)

> **Hinweis:** Java, Maven, Azure CLI und Azure Developer CLI (azd) sind im bereitgestellten Devcontainer vorinstalliert.

> **Hinweis:** Dieses Modul verwendet GPT-5 auf Azure OpenAI. Die Bereitstellung wird automatisch √ºber `azd up` konfiguriert ‚Äì √§ndern Sie den Modellnamen im Code nicht.

## Das Kernproblem verstehen

Sprachmodelle sind zustandslos. Jeder API-Aufruf ist unabh√§ngig. Wenn Sie "Mein Name ist John" senden und dann fragen "Wie hei√üt du?", hat das Modell keine Ahnung, dass Sie sich gerade vorgestellt haben. Es behandelt jede Anfrage, als w√§re es das erste Gespr√§ch, das Sie je gef√ºhrt haben.

Das ist f√ºr einfache Fragen und Antworten in Ordnung, aber f√ºr echte Anwendungen nutzlos. Kundenservice-Bots m√ºssen sich merken, was Sie ihnen gesagt haben. Pers√∂nliche Assistenten brauchen Kontext. Jede mehrstufige Konversation erfordert Speicher.

<img src="../../../translated_images/stateless-vs-stateful.cc4a4765e649c41a.de.png" alt="Zustandslose vs. zustandsbehaftete Konversationen" width="800"/>

*Der Unterschied zwischen zustandslosen (unabh√§ngigen Aufrufen) und zustandsbehafteten (kontextbewussten) Konversationen*

## Tokens verstehen

Bevor wir in Konversationen eintauchen, ist es wichtig, Tokens zu verstehen ‚Äì die Grundeinheiten des Textes, die Sprachmodelle verarbeiten:

<img src="../../../translated_images/token-explanation.c39760d8ec650181.de.png" alt="Token-Erkl√§rung" width="800"/>

*Beispiel, wie Text in Tokens zerlegt wird ‚Äì "I love AI!" wird zu 4 separaten Verarbeitungseinheiten*

Tokens sind die Ma√üeinheit, mit der KI-Modelle Text messen und verarbeiten. W√∂rter, Satzzeichen und sogar Leerzeichen k√∂nnen Tokens sein. Ihr Modell hat eine Grenze, wie viele Tokens es gleichzeitig verarbeiten kann (400.000 f√ºr GPT-5, mit bis zu 272.000 Eingabe-Tokens und 128.000 Ausgabe-Tokens). Das Verst√§ndnis von Tokens hilft Ihnen, die L√§nge der Konversation und die Kosten zu steuern.

## Wie Speicher funktioniert

Chat-Speicher l√∂st das Problem der Zustandslosigkeit, indem er den Gespr√§chsverlauf beibeh√§lt. Bevor Ihre Anfrage an das Modell gesendet wird, f√ºgt das Framework relevante vorherige Nachrichten hinzu. Wenn Sie fragen "Wie hei√üt du?", sendet das System tats√§chlich den gesamten Gespr√§chsverlauf, sodass das Modell sieht, dass Sie zuvor "Mein Name ist John" gesagt haben.

LangChain4j bietet Speicherimplementierungen, die dies automatisch handhaben. Sie w√§hlen, wie viele Nachrichten behalten werden sollen, und das Framework verwaltet das Kontextfenster.

<img src="../../../translated_images/memory-window.bbe67f597eadabb3.de.png" alt="Speicherfenster-Konzept" width="800"/>

*MessageWindowChatMemory h√§lt ein gleitendes Fenster der letzten Nachrichten und entfernt automatisch alte*

## Wie dies LangChain4j verwendet

Dieses Modul erweitert den Schnellstart, indem es Spring Boot integriert und Gespr√§chsspeicher hinzuf√ºgt. So f√ºgen sich die Teile zusammen:

**Abh√§ngigkeiten** ‚Äì F√ºgen Sie zwei LangChain4j-Bibliotheken hinzu:

```xml
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-open-ai-official</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
```

**Chat-Modell** ‚Äì Konfigurieren Sie Azure OpenAI als Spring-Bean ([LangChainConfig.java](../../../01-introduction/src/main/java/com/example/langchain4j/config/LangChainConfig.java)):

```java
@Bean
public OpenAiOfficialChatModel openAiOfficialChatModel() {
    return OpenAiOfficialChatModel.builder()
            .baseUrl(azureEndpoint)
            .apiKey(azureApiKey)
            .modelName(deploymentName)
            .timeout(Duration.ofMinutes(5))
            .maxRetries(3)
            .build();
}
```

Der Builder liest Anmeldeinformationen aus Umgebungsvariablen, die von `azd up` gesetzt werden. Das Setzen von `baseUrl` auf Ihren Azure-Endpunkt l√§sst den OpenAI-Client mit Azure OpenAI arbeiten.

**Gespr√§chsspeicher** ‚Äì Verfolgen Sie den Chat-Verlauf mit MessageWindowChatMemory ([ConversationService.java](../../../01-introduction/src/main/java/com/example/langchain4j/service/ConversationService.java)):

```java
ChatMemory memory = MessageWindowChatMemory.withMaxMessages(10);

memory.add(UserMessage.from("My name is John"));
memory.add(AiMessage.from("Nice to meet you, John!"));

memory.add(UserMessage.from("What's my name?"));
AiMessage aiMessage = chatModel.chat(memory.messages()).aiMessage();
memory.add(aiMessage);
```

Erstellen Sie Speicher mit `withMaxMessages(10)`, um die letzten 10 Nachrichten zu behalten. F√ºgen Sie Benutzer- und KI-Nachrichten mit typisierten Wrappern hinzu: `UserMessage.from(text)` und `AiMessage.from(text)`. Rufen Sie den Verlauf mit `memory.messages()` ab und senden Sie ihn an das Modell. Der Service speichert separate Speicherinstanzen pro Gespr√§chs-ID, sodass mehrere Benutzer gleichzeitig chatten k√∂nnen.

> **ü§ñ Probieren Sie es mit [GitHub Copilot](https://github.com/features/copilot) Chat:** √ñffnen Sie [`ConversationService.java`](../../../01-introduction/src/main/java/com/example/langchain4j/service/ConversationService.java) und fragen Sie:
> - "Wie entscheidet MessageWindowChatMemory, welche Nachrichten verworfen werden, wenn das Fenster voll ist?"
> - "Kann ich benutzerdefinierten Speicher mit einer Datenbank anstelle von In-Memory implementieren?"
> - "Wie w√ºrde ich eine Zusammenfassung hinzuf√ºgen, um alten Gespr√§chsverlauf zu komprimieren?"

Der zustandslose Chat-Endpunkt √ºberspringt Speicher vollst√§ndig ‚Äì einfach `chatModel.chat(prompt)` wie im Schnellstart. Der zustandsbehaftete Endpunkt f√ºgt Nachrichten zum Speicher hinzu, ruft den Verlauf ab und schickt diesen Kontext mit jeder Anfrage. Gleiche Modellkonfiguration, unterschiedliche Muster.

## Azure OpenAI Infrastruktur bereitstellen

**Bash:**
```bash
cd 01-introduction
azd up  # W√§hlen Sie das Abonnement und den Standort (eastus2 empfohlen)
```

**PowerShell:**
```powershell
cd 01-introduction
azd up  # W√§hlen Sie das Abonnement und den Standort (eastus2 empfohlen)
```

> **Hinweis:** Wenn Sie einen Timeout-Fehler erhalten (`RequestConflict: Cannot modify resource ... provisioning state is not terminal`), f√ºhren Sie einfach `azd up` erneut aus. Azure-Ressourcen werden m√∂glicherweise noch im Hintergrund bereitgestellt, und ein erneuter Versuch erm√∂glicht die Fertigstellung, sobald die Ressourcen einen Endzustand erreichen.

Dies wird:
1. Azure OpenAI-Ressource mit GPT-5 und text-embedding-3-small Modellen bereitstellen
2. Automatisch eine `.env`-Datei im Projektstamm mit Anmeldeinformationen generieren
3. Alle erforderlichen Umgebungsvariablen einrichten

**Probleme bei der Bereitstellung?** Siehe das [Infrastructure README](infra/README.md) f√ºr detaillierte Fehlerbehebung, einschlie√ülich Konflikten bei Subdomain-Namen, manuellen Azure-Portal-Bereitstellungsschritten und Modellkonfigurationsanleitungen.

**Bereitstellungserfolg √ºberpr√ºfen:**

**Bash:**
```bash
cat ../.env  # Sollte AZURE_OPENAI_ENDPOINT, API_KEY usw. anzeigen.
```

**PowerShell:**
```powershell
Get-Content ..\.env  # Sollte AZURE_OPENAI_ENDPOINT, API_KEY usw. anzeigen.
```

> **Hinweis:** Der Befehl `azd up` generiert automatisch die `.env`-Datei. Wenn Sie diese sp√§ter aktualisieren m√ºssen, k√∂nnen Sie entweder die `.env`-Datei manuell bearbeiten oder sie durch Ausf√ºhren von:
>
> **Bash:**
> ```bash
> cd ..
> bash .azd-env.sh
> ```
>
> **PowerShell:**
> ```powershell
> cd ..
> .\.azd-env.ps1
> ```

## Die Anwendung lokal ausf√ºhren

**Bereitstellung √ºberpr√ºfen:**

Stellen Sie sicher, dass die `.env`-Datei im Stammverzeichnis mit Azure-Anmeldeinformationen vorhanden ist:

**Bash:**
```bash
cat ../.env  # Sollte AZURE_OPENAI_ENDPOINT, API_KEY, DEPLOYMENT anzeigen
```

**PowerShell:**
```powershell
Get-Content ..\.env  # Sollte AZURE_OPENAI_ENDPOINT, API_KEY, DEPLOYMENT anzeigen
```

**Starten Sie die Anwendungen:**

**Option 1: Verwendung des Spring Boot Dashboards (empfohlen f√ºr VS Code Nutzer)**

Der Devcontainer enth√§lt die Spring Boot Dashboard-Erweiterung, die eine visuelle Oberfl√§che zur Verwaltung aller Spring Boot-Anwendungen bietet. Sie finden sie in der Aktivit√§tsleiste links in VS Code (suchen Sie nach dem Spring Boot-Symbol).

Im Spring Boot Dashboard k√∂nnen Sie:
- Alle verf√ºgbaren Spring Boot-Anwendungen im Arbeitsbereich sehen
- Anwendungen mit einem Klick starten/stoppen
- Anwendungsprotokolle in Echtzeit ansehen
- Anwendungsstatus √ºberwachen

Klicken Sie einfach auf den Play-Button neben "introduction", um dieses Modul zu starten, oder starten Sie alle Module gleichzeitig.

<img src="../../../translated_images/dashboard.69c7479aef09ff6b.de.png" alt="Spring Boot Dashboard" width="400"/>

**Option 2: Verwendung von Shell-Skripten**

Starten Sie alle Webanwendungen (Module 01-04):

**Bash:**
```bash
cd ..  # Vom Stammverzeichnis
./start-all.sh
```

**PowerShell:**
```powershell
cd ..  # Vom Stammverzeichnis
.\start-all.ps1
```

Oder starten Sie nur dieses Modul:

**Bash:**
```bash
cd 01-introduction
./start.sh
```

**PowerShell:**
```powershell
cd 01-introduction
.\start.ps1
```

Beide Skripte laden automatisch Umgebungsvariablen aus der `.env`-Datei im Stammverzeichnis und bauen die JARs, falls sie nicht existieren.

> **Hinweis:** Wenn Sie alle Module manuell vor dem Start bauen m√∂chten:
>
> **Bash:**
> ```bash
> cd ..  # Go to root directory
> mvn clean package -DskipTests
> ```
>
> **PowerShell:**
> ```powershell
> cd ..  # Go to root directory
> mvn clean package -DskipTests
> ```

√ñffnen Sie http://localhost:8080 in Ihrem Browser.

**Zum Stoppen:**

**Bash:**
```bash
./stop.sh  # Nur dieses Modul
# Oder
cd .. && ./stop-all.sh  # Alle Module
```

**PowerShell:**
```powershell
.\stop.ps1  # Nur dieses Modul
# Oder
cd ..; .\stop-all.ps1  # Alle Module
```

## Die Anwendung verwenden

Die Anwendung bietet eine Weboberfl√§che mit zwei Chat-Implementierungen nebeneinander.

<img src="../../../translated_images/home-screen.121a03206ab910c0.de.png" alt="Startbildschirm der Anwendung" width="800"/>

*Dashboard zeigt sowohl Simple Chat (zustandslos) als auch Conversational Chat (zustandsbehaftet) Optionen*

### Zustandsloser Chat (linkes Panel)

Probieren Sie dies zuerst. Fragen Sie "Mein Name ist John" und dann sofort "Wie hei√üt du?" Das Modell wird sich nicht erinnern, da jede Nachricht unabh√§ngig ist. Dies zeigt das Kernproblem bei der einfachen Integration von Sprachmodellen ‚Äì kein Gespr√§chskontext.

<img src="../../../translated_images/simple-chat-stateless-demo.13aeb3978eab3234.de.png" alt="Demo Zustandsloser Chat" width="800"/>

*Die KI erinnert sich nicht an Ihren Namen aus der vorherigen Nachricht*

### Zustandsbehafteter Chat (rechtes Panel)

Probieren Sie hier dieselbe Abfolge. Fragen Sie "Mein Name ist John" und dann "Wie hei√üt du?" Diesmal erinnert es sich. Der Unterschied ist MessageWindowChatMemory ‚Äì es h√§lt den Gespr√§chsverlauf und f√ºgt ihn jeder Anfrage hinzu. So funktioniert produktionsreife konversationelle KI.

<img src="../../../translated_images/conversational-chat-stateful-demo.e5be9822eb23ff59.de.png" alt="Demo Zustandsbehafteter Chat" width="800"/>

*Die KI erinnert sich an Ihren Namen aus dem fr√ºheren Gespr√§ch*

Beide Panels verwenden dasselbe GPT-5-Modell. Der einzige Unterschied ist der Speicher. Das macht deutlich, was Speicher Ihrer Anwendung bringt und warum er f√ºr reale Anwendungsf√§lle unerl√§sslich ist.

## N√§chste Schritte

**N√§chstes Modul:** [02-prompt-engineering - Prompt Engineering mit GPT-5](../02-prompt-engineering/README.md)

---

**Navigation:** [‚Üê Vorheriges: Modul 00 - Schnellstart](../00-quick-start/README.md) | [Zur√ºck zur √úbersicht](../README.md) | [N√§chstes: Modul 02 - Prompt Engineering ‚Üí](../02-prompt-engineering/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner Ursprungssprache ist als ma√ügebliche Quelle zu betrachten. F√ºr wichtige Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Nutzung dieser √úbersetzung entstehen.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->