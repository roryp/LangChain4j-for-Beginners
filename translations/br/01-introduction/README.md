<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c3e07ca58d0b8a3f47d3bf5728541e0a",
  "translation_date": "2025-12-13T13:30:35+00:00",
  "source_file": "01-introduction/README.md",
  "language_code": "br"
}
-->
# M√≥dulo 01: Come√ßando com LangChain4j

## √çndice

- [O que voc√™ vai aprender](../../../01-introduction)
- [Pr√©-requisitos](../../../01-introduction)
- [Entendendo o problema central](../../../01-introduction)
- [Entendendo tokens](../../../01-introduction)
- [Como a mem√≥ria funciona](../../../01-introduction)
- [Como isso usa LangChain4j](../../../01-introduction)
- [Implantar infraestrutura Azure OpenAI](../../../01-introduction)
- [Executar a aplica√ß√£o localmente](../../../01-introduction)
- [Usando a aplica√ß√£o](../../../01-introduction)
  - [Chat sem estado (painel esquerdo)](../../../01-introduction)
  - [Chat com estado (painel direito)](../../../01-introduction)
- [Pr√≥ximos passos](../../../01-introduction)

## O que voc√™ vai aprender

Se voc√™ completou o in√≠cio r√°pido, viu como enviar prompts e obter respostas. Essa √© a base, mas aplica√ß√µes reais precisam de mais. Este m√≥dulo ensina como construir IA conversacional que lembra o contexto e mant√©m estado - a diferen√ßa entre uma demonstra√ß√£o pontual e uma aplica√ß√£o pronta para produ√ß√£o.

Usaremos o GPT-5 do Azure OpenAI ao longo deste guia porque suas capacidades avan√ßadas de racioc√≠nio tornam o comportamento dos diferentes padr√µes mais evidente. Quando voc√™ adiciona mem√≥ria, ver√° claramente a diferen√ßa. Isso facilita entender o que cada componente traz para sua aplica√ß√£o.

Voc√™ construir√° uma aplica√ß√£o que demonstra ambos os padr√µes:

**Chat sem estado** - Cada requisi√ß√£o √© independente. O modelo n√£o tem mem√≥ria das mensagens anteriores. Este √© o padr√£o que voc√™ usou no in√≠cio r√°pido.

**Conversa com estado** - Cada requisi√ß√£o inclui o hist√≥rico da conversa. O modelo mant√©m o contexto em m√∫ltiplas intera√ß√µes. √â isso que aplica√ß√µes de produ√ß√£o exigem.

## Pr√©-requisitos

- Assinatura Azure com acesso ao Azure OpenAI
- Java 21, Maven 3.9+
- Azure CLI (https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)
- Azure Developer CLI (azd) (https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd)

> **Nota:** Java, Maven, Azure CLI e Azure Developer CLI (azd) j√° est√£o pr√©-instalados no devcontainer fornecido.

> **Nota:** Este m√≥dulo usa GPT-5 no Azure OpenAI. A implanta√ß√£o √© configurada automaticamente via `azd up` - n√£o modifique o nome do modelo no c√≥digo.

## Entendendo o problema central

Modelos de linguagem s√£o sem estado. Cada chamada de API √© independente. Se voc√™ enviar "Meu nome √© Jo√£o" e depois perguntar "Qual √© o meu nome?", o modelo n√£o tem ideia de que voc√™ acabou de se apresentar. Ele trata cada requisi√ß√£o como se fosse a primeira conversa que voc√™ j√° teve.

Isso √© aceit√°vel para perguntas e respostas simples, mas in√∫til para aplica√ß√µes reais. Bots de atendimento ao cliente precisam lembrar o que voc√™ disse. Assistentes pessoais precisam de contexto. Qualquer conversa com m√∫ltiplas intera√ß√µes requer mem√≥ria.

<img src="../../../translated_images/br/stateless-vs-stateful.cc4a4765e649c41a.png" alt="Conversas sem estado vs com estado" width="800"/>

*A diferen√ßa entre conversas sem estado (chamadas independentes) e com estado (cientes do contexto)*

## Entendendo tokens

Antes de mergulhar nas conversas, √© importante entender tokens - as unidades b√°sicas de texto que os modelos de linguagem processam:

<img src="../../../translated_images/br/token-explanation.c39760d8ec650181.png" alt="Explica√ß√£o de token" width="800"/>

*Exemplo de como o texto √© dividido em tokens - "Eu amo IA!" vira 4 unidades separadas para processamento*

Tokens s√£o como os modelos de IA medem e processam texto. Palavras, pontua√ß√£o e at√© espa√ßos podem ser tokens. Seu modelo tem um limite de quantos tokens pode processar de uma vez (400.000 para GPT-5, com at√© 272.000 tokens de entrada e 128.000 tokens de sa√≠da). Entender tokens ajuda a gerenciar o tamanho da conversa e os custos.

## Como a mem√≥ria funciona

A mem√≥ria do chat resolve o problema de ser sem estado mantendo o hist√≥rico da conversa. Antes de enviar sua requisi√ß√£o ao modelo, o framework antep√µe mensagens anteriores relevantes. Quando voc√™ pergunta "Qual √© o meu nome?", o sistema na verdade envia todo o hist√≥rico da conversa, permitindo que o modelo veja que voc√™ disse antes "Meu nome √© Jo√£o."

LangChain4j fornece implementa√ß√µes de mem√≥ria que fazem isso automaticamente. Voc√™ escolhe quantas mensagens manter e o framework gerencia a janela de contexto.

<img src="../../../translated_images/br/memory-window.bbe67f597eadabb3.png" alt="Conceito de janela de mem√≥ria" width="800"/>

*MessageWindowChatMemory mant√©m uma janela deslizante das mensagens recentes, descartando automaticamente as antigas*

## Como isso usa LangChain4j

Este m√≥dulo estende o in√≠cio r√°pido integrando Spring Boot e adicionando mem√≥ria de conversa. Veja como as pe√ßas se encaixam:

**Depend√™ncias** - Adicione duas bibliotecas LangChain4j:

```xml
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-open-ai-official</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
```

**Modelo de Chat** - Configure Azure OpenAI como um bean Spring ([LangChainConfig.java](../../../01-introduction/src/main/java/com/example/langchain4j/config/LangChainConfig.java)):

```java
@Bean
public OpenAiOfficialChatModel openAiOfficialChatModel() {
    return OpenAiOfficialChatModel.builder()
            .baseUrl(azureEndpoint)
            .apiKey(azureApiKey)
            .modelName(deploymentName)
            .timeout(Duration.ofMinutes(5))
            .maxRetries(3)
            .build();
}
```

O builder l√™ as credenciais das vari√°veis de ambiente definidas pelo `azd up`. Definir `baseUrl` para seu endpoint Azure faz o cliente OpenAI funcionar com Azure OpenAI.

**Mem√≥ria da Conversa** - Acompanhe o hist√≥rico do chat com MessageWindowChatMemory ([ConversationService.java](../../../01-introduction/src/main/java/com/example/langchain4j/service/ConversationService.java)):

```java
ChatMemory memory = MessageWindowChatMemory.withMaxMessages(10);

memory.add(UserMessage.from("My name is John"));
memory.add(AiMessage.from("Nice to meet you, John!"));

memory.add(UserMessage.from("What's my name?"));
AiMessage aiMessage = chatModel.chat(memory.messages()).aiMessage();
memory.add(aiMessage);
```

Crie a mem√≥ria com `withMaxMessages(10)` para manter as √∫ltimas 10 mensagens. Adicione mensagens do usu√°rio e da IA com wrappers tipados: `UserMessage.from(text)` e `AiMessage.from(text)`. Recupere o hist√≥rico com `memory.messages()` e envie para o modelo. O servi√ßo armazena inst√¢ncias de mem√≥ria separadas por ID de conversa, permitindo m√∫ltiplos usu√°rios conversarem simultaneamente.

> **ü§ñ Experimente com o [GitHub Copilot](https://github.com/features/copilot) Chat:** Abra [`ConversationService.java`](../../../01-introduction/src/main/java/com/example/langchain4j/service/ConversationService.java) e pergunte:
> - "Como o MessageWindowChatMemory decide quais mensagens descartar quando a janela est√° cheia?"
> - "Posso implementar armazenamento de mem√≥ria personalizado usando um banco de dados em vez de mem√≥ria em RAM?"
> - "Como eu adicionaria sumariza√ß√£o para comprimir o hist√≥rico antigo da conversa?"

O endpoint de chat sem estado ignora a mem√≥ria completamente - apenas `chatModel.chat(prompt)` como no in√≠cio r√°pido. O endpoint com estado adiciona mensagens √† mem√≥ria, recupera o hist√≥rico e inclui esse contexto em cada requisi√ß√£o. Mesma configura√ß√£o de modelo, padr√µes diferentes.

## Implantar infraestrutura Azure OpenAI

**Bash:**
```bash
cd 01-introduction
azd up  # Selecione a assinatura e a localiza√ß√£o (eastus2 recomendado)
```

**PowerShell:**
```powershell
cd 01-introduction
azd up  # Selecione a assinatura e a localiza√ß√£o (eastus2 recomendado)
```

> **Nota:** Se voc√™ encontrar um erro de timeout (`RequestConflict: Cannot modify resource ... provisioning state is not terminal`), simplesmente execute `azd up` novamente. Os recursos do Azure podem ainda estar sendo provisionados em segundo plano, e tentar novamente permite que a implanta√ß√£o seja conclu√≠da assim que os recursos atingirem um estado terminal.

Isso ir√°:
1. Implantar recurso Azure OpenAI com modelos GPT-5 e text-embedding-3-small
2. Gerar automaticamente o arquivo `.env` na raiz do projeto com as credenciais
3. Configurar todas as vari√°veis de ambiente necess√°rias

**Est√° tendo problemas na implanta√ß√£o?** Veja o [README da infraestrutura](infra/README.md) para solu√ß√£o detalhada incluindo conflitos de nome de subdom√≠nio, passos manuais no Portal Azure e orienta√ß√µes de configura√ß√£o do modelo.

**Verifique se a implanta√ß√£o foi bem-sucedida:**

**Bash:**
```bash
cat ../.env  # Deve mostrar AZURE_OPENAI_ENDPOINT, API_KEY, etc.
```

**PowerShell:**
```powershell
Get-Content ..\.env  # Deve mostrar AZURE_OPENAI_ENDPOINT, API_KEY, etc.
```

> **Nota:** O comando `azd up` gera automaticamente o arquivo `.env`. Se precisar atualiz√°-lo depois, voc√™ pode editar o arquivo `.env` manualmente ou regener√°-lo executando:
>
> **Bash:**
> ```bash
> cd ..
> bash .azd-env.sh
> ```
>
> **PowerShell:**
> ```powershell
> cd ..
> .\.azd-env.ps1
> ```

## Executar a aplica√ß√£o localmente

**Verifique a implanta√ß√£o:**

Certifique-se de que o arquivo `.env` existe no diret√≥rio raiz com as credenciais Azure:

**Bash:**
```bash
cat ../.env  # Deve mostrar AZURE_OPENAI_ENDPOINT, API_KEY, DEPLOYMENT
```

**PowerShell:**
```powershell
Get-Content ..\.env  # Deve mostrar AZURE_OPENAI_ENDPOINT, API_KEY, DEPLOYMENT
```

**Inicie as aplica√ß√µes:**

**Op√ß√£o 1: Usando o Spring Boot Dashboard (Recomendado para usu√°rios VS Code)**

O dev container inclui a extens√£o Spring Boot Dashboard, que fornece uma interface visual para gerenciar todas as aplica√ß√µes Spring Boot. Voc√™ pode encontr√°-la na Barra de Atividades √† esquerda do VS Code (procure o √≠cone do Spring Boot).

No Spring Boot Dashboard, voc√™ pode:
- Ver todas as aplica√ß√µes Spring Boot dispon√≠veis no workspace
- Iniciar/parar aplica√ß√µes com um clique
- Visualizar logs da aplica√ß√£o em tempo real
- Monitorar o status da aplica√ß√£o

Basta clicar no bot√£o de play ao lado de "introduction" para iniciar este m√≥dulo, ou iniciar todos os m√≥dulos de uma vez.

<img src="../../../translated_images/br/dashboard.69c7479aef09ff6b.png" alt="Spring Boot Dashboard" width="400"/>

**Op√ß√£o 2: Usando scripts shell**

Inicie todas as aplica√ß√µes web (m√≥dulos 01-04):

**Bash:**
```bash
cd ..  # A partir do diret√≥rio raiz
./start-all.sh
```

**PowerShell:**
```powershell
cd ..  # A partir do diret√≥rio raiz
.\start-all.ps1
```

Ou inicie apenas este m√≥dulo:

**Bash:**
```bash
cd 01-introduction
./start.sh
```

**PowerShell:**
```powershell
cd 01-introduction
.\start.ps1
```

Ambos os scripts carregam automaticamente as vari√°veis de ambiente do arquivo `.env` raiz e ir√£o construir os JARs se eles n√£o existirem.

> **Nota:** Se preferir construir todos os m√≥dulos manualmente antes de iniciar:
>
> **Bash:**
> ```bash
> cd ..  # Go to root directory
> mvn clean package -DskipTests
> ```
>
> **PowerShell:**
> ```powershell
> cd ..  # Go to root directory
> mvn clean package -DskipTests
> ```

Abra http://localhost:8080 no seu navegador.

**Para parar:**

**Bash:**
```bash
./stop.sh  # Apenas este m√≥dulo
# Ou
cd .. && ./stop-all.sh  # Todos os m√≥dulos
```

**PowerShell:**
```powershell
.\stop.ps1  # Apenas este m√≥dulo
# Ou
cd ..; .\stop-all.ps1  # Todos os m√≥dulos
```

## Usando a aplica√ß√£o

A aplica√ß√£o fornece uma interface web com duas implementa√ß√µes de chat lado a lado.

<img src="../../../translated_images/br/home-screen.121a03206ab910c0.png" alt="Tela inicial da aplica√ß√£o" width="800"/>

*Dashboard mostrando as op√ß√µes de Chat Simples (sem estado) e Chat Conversacional (com estado)*

### Chat sem estado (painel esquerdo)

Experimente primeiro aqui. Pergunte "Meu nome √© Jo√£o" e logo em seguida "Qual √© o meu nome?" O modelo n√£o vai lembrar porque cada mensagem √© independente. Isso demonstra o problema central da integra√ß√£o b√°sica com modelos de linguagem - sem contexto de conversa.

<img src="../../../translated_images/br/simple-chat-stateless-demo.13aeb3978eab3234.png" alt="Demonstra√ß√£o de chat sem estado" width="800"/>

*A IA n√£o lembra seu nome da mensagem anterior*

### Chat com estado (painel direito)

Agora tente a mesma sequ√™ncia aqui. Pergunte "Meu nome √© Jo√£o" e depois "Qual √© o meu nome?" Desta vez ela lembra. A diferen√ßa √© o MessageWindowChatMemory - ele mant√©m o hist√≥rico da conversa e o inclui em cada requisi√ß√£o. √â assim que a IA conversacional de produ√ß√£o funciona.

<img src="../../../translated_images/br/conversational-chat-stateful-demo.e5be9822eb23ff59.png" alt="Demonstra√ß√£o de chat com estado" width="800"/>

*A IA lembra seu nome de antes na conversa*

Ambos os pain√©is usam o mesmo modelo GPT-5. A √∫nica diferen√ßa √© a mem√≥ria. Isso deixa claro o que a mem√≥ria traz para sua aplica√ß√£o e por que √© essencial para casos de uso reais.

## Pr√≥ximos passos

**Pr√≥ximo m√≥dulo:** [02-prompt-engineering - Engenharia de Prompt com GPT-5](../02-prompt-engineering/README.md)

---

**Navega√ß√£o:** [‚Üê Anterior: M√≥dulo 00 - In√≠cio R√°pido](../00-quick-start/README.md) | [Voltar ao In√≠cio](../README.md) | [Pr√≥ximo: M√≥dulo 02 - Engenharia de Prompt ‚Üí](../02-prompt-engineering/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informa√ß√µes cr√≠ticas, recomenda-se tradu√ß√£o profissional humana. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->