<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c3e07ca58d0b8a3f47d3bf5728541e0a",
  "translation_date": "2025-12-13T13:29:04+00:00",
  "source_file": "01-introduction/README.md",
  "language_code": "pt"
}
-->
# M√≥dulo 01: Come√ßar com LangChain4j

## √çndice

- [O que vai aprender](../../../01-introduction)
- [Pr√©-requisitos](../../../01-introduction)
- [Compreender o Problema Central](../../../01-introduction)
- [Compreender Tokens](../../../01-introduction)
- [Como a Mem√≥ria Funciona](../../../01-introduction)
- [Como Isto Usa LangChain4j](../../../01-introduction)
- [Desplegar Infraestrutura Azure OpenAI](../../../01-introduction)
- [Executar a Aplica√ß√£o Localmente](../../../01-introduction)
- [Usar a Aplica√ß√£o](../../../01-introduction)
  - [Chat Sem Estado (Painel Esquerdo)](../../../01-introduction)
  - [Chat Com Estado (Painel Direito)](../../../01-introduction)
- [Pr√≥ximos Passos](../../../01-introduction)

## O que vai aprender

Se completou o in√≠cio r√°pido, viu como enviar prompts e obter respostas. Essa √© a base, mas aplica√ß√µes reais precisam de mais. Este m√≥dulo ensina como construir IA conversacional que lembra o contexto e mant√©m o estado - a diferen√ßa entre uma demo pontual e uma aplica√ß√£o pronta para produ√ß√£o.

Vamos usar o GPT-5 da Azure OpenAI ao longo deste guia porque as suas capacidades avan√ßadas de racioc√≠nio tornam o comportamento dos diferentes padr√µes mais evidente. Quando adiciona mem√≥ria, ver√° claramente a diferen√ßa. Isto facilita a compreens√£o do que cada componente traz para a sua aplica√ß√£o.

Vai construir uma aplica√ß√£o que demonstra ambos os padr√µes:

**Chat Sem Estado** - Cada pedido √© independente. O modelo n√£o tem mem√≥ria das mensagens anteriores. Este √© o padr√£o que usou no in√≠cio r√°pido.

**Conversa Com Estado** - Cada pedido inclui o hist√≥rico da conversa. O modelo mant√©m o contexto ao longo de m√∫ltiplas intera√ß√µes. Isto √© o que as aplica√ß√µes de produ√ß√£o exigem.

## Pr√©-requisitos

- Subscri√ß√£o Azure com acesso ao Azure OpenAI
- Java 21, Maven 3.9+
- Azure CLI (https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)
- Azure Developer CLI (azd) (https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd)

> **Nota:** Java, Maven, Azure CLI e Azure Developer CLI (azd) est√£o pr√©-instalados no devcontainer fornecido.

> **Nota:** Este m√≥dulo usa GPT-5 no Azure OpenAI. O deployment √© configurado automaticamente via `azd up` - n√£o modifique o nome do modelo no c√≥digo.

## Compreender o Problema Central

Modelos de linguagem s√£o sem estado. Cada chamada √† API √© independente. Se enviar "O meu nome √© Jo√£o" e depois perguntar "Qual √© o meu nome?", o modelo n√£o tem ideia que acabou de se apresentar. Trata cada pedido como se fosse a primeira conversa que alguma vez teve.

Isto √© aceit√°vel para perguntas e respostas simples, mas in√∫til para aplica√ß√µes reais. Bots de servi√ßo ao cliente precisam lembrar o que lhes disse. Assistentes pessoais precisam de contexto. Qualquer conversa com m√∫ltiplas intera√ß√µes requer mem√≥ria.

<img src="../../../translated_images/pt-PT/stateless-vs-stateful.cc4a4765e649c41a.webp" alt="Conversas Sem Estado vs Com Estado" width="800"/>

*A diferen√ßa entre conversas sem estado (chamadas independentes) e com estado (sens√≠veis ao contexto)*

## Compreender Tokens

Antes de mergulhar nas conversas, √© importante entender tokens - as unidades b√°sicas de texto que os modelos de linguagem processam:

<img src="../../../translated_images/pt-PT/token-explanation.c39760d8ec650181.webp" alt="Explica√ß√£o de Token" width="800"/>

*Exemplo de como o texto √© dividido em tokens - "I love AI!" torna-se 4 unidades de processamento separadas*

Tokens s√£o como os modelos de IA medem e processam texto. Palavras, pontua√ß√£o e at√© espa√ßos podem ser tokens. O seu modelo tem um limite de quantos tokens pode processar de uma vez (400.000 para GPT-5, com at√© 272.000 tokens de entrada e 128.000 tokens de sa√≠da). Compreender tokens ajuda a gerir o comprimento da conversa e os custos.

## Como a Mem√≥ria Funciona

A mem√≥ria de chat resolve o problema de ser sem estado ao manter o hist√≥rico da conversa. Antes de enviar o seu pedido ao modelo, o framework antep√µe mensagens anteriores relevantes. Quando pergunta "Qual √© o meu nome?", o sistema envia na verdade todo o hist√≥rico da conversa, permitindo que o modelo veja que disse anteriormente "O meu nome √© Jo√£o."

LangChain4j fornece implementa√ß√µes de mem√≥ria que tratam disto automaticamente. Escolhe quantas mensagens reter e o framework gere a janela de contexto.

<img src="../../../translated_images/pt-PT/memory-window.bbe67f597eadabb3.webp" alt="Conceito de Janela de Mem√≥ria" width="800"/>

*MessageWindowChatMemory mant√©m uma janela deslizante das mensagens recentes, descartando automaticamente as antigas*

## Como Isto Usa LangChain4j

Este m√≥dulo estende o in√≠cio r√°pido integrando Spring Boot e adicionando mem√≥ria de conversa. Eis como as pe√ßas se encaixam:

**Depend√™ncias** - Adicione duas bibliotecas LangChain4j:

```xml
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-open-ai-official</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
```

**Modelo de Chat** - Configure Azure OpenAI como um bean Spring ([LangChainConfig.java](../../../01-introduction/src/main/java/com/example/langchain4j/config/LangChainConfig.java)):

```java
@Bean
public OpenAiOfficialChatModel openAiOfficialChatModel() {
    return OpenAiOfficialChatModel.builder()
            .baseUrl(azureEndpoint)
            .apiKey(azureApiKey)
            .modelName(deploymentName)
            .timeout(Duration.ofMinutes(5))
            .maxRetries(3)
            .build();
}
```

O builder l√™ as credenciais das vari√°veis de ambiente definidas pelo `azd up`. Definir `baseUrl` para o seu endpoint Azure faz o cliente OpenAI funcionar com Azure OpenAI.

**Mem√≥ria de Conversa** - Acompanhe o hist√≥rico do chat com MessageWindowChatMemory ([ConversationService.java](../../../01-introduction/src/main/java/com/example/langchain4j/service/ConversationService.java)):

```java
ChatMemory memory = MessageWindowChatMemory.withMaxMessages(10);

memory.add(UserMessage.from("My name is John"));
memory.add(AiMessage.from("Nice to meet you, John!"));

memory.add(UserMessage.from("What's my name?"));
AiMessage aiMessage = chatModel.chat(memory.messages()).aiMessage();
memory.add(aiMessage);
```

Crie a mem√≥ria com `withMaxMessages(10)` para manter as √∫ltimas 10 mensagens. Adicione mensagens do utilizador e da IA com wrappers tipados: `UserMessage.from(text)` e `AiMessage.from(text)`. Recupere o hist√≥rico com `memory.messages()` e envie-o ao modelo. O servi√ßo armazena inst√¢ncias de mem√≥ria separadas por ID de conversa, permitindo que m√∫ltiplos utilizadores conversem simultaneamente.

> **ü§ñ Experimente com o [GitHub Copilot](https://github.com/features/copilot) Chat:** Abra [`ConversationService.java`](../../../01-introduction/src/main/java/com/example/langchain4j/service/ConversationService.java) e pergunte:
> - "Como √© que o MessageWindowChatMemory decide quais mensagens descartar quando a janela est√° cheia?"
> - "Posso implementar armazenamento de mem√≥ria personalizado usando uma base de dados em vez de mem√≥ria em RAM?"
> - "Como adicionaria sumariza√ß√£o para comprimir o hist√≥rico antigo da conversa?"

O endpoint de chat sem estado ignora a mem√≥ria completamente - apenas `chatModel.chat(prompt)` como no in√≠cio r√°pido. O endpoint com estado adiciona mensagens √† mem√≥ria, recupera o hist√≥rico e inclui esse contexto em cada pedido. Mesma configura√ß√£o de modelo, padr√µes diferentes.

## Desplegar Infraestrutura Azure OpenAI

**Bash:**
```bash
cd 01-introduction
azd up  # Selecionar subscri√ß√£o e localiza√ß√£o (eastus2 recomendado)
```

**PowerShell:**
```powershell
cd 01-introduction
azd up  # Selecione a subscri√ß√£o e a localiza√ß√£o (eastus2 recomendado)
```

> **Nota:** Se encontrar um erro de timeout (`RequestConflict: Cannot modify resource ... provisioning state is not terminal`), simplesmente execute `azd up` novamente. Os recursos Azure podem ainda estar a ser provisionados em segundo plano, e tentar novamente permite que o deployment seja conclu√≠do assim que os recursos atingirem um estado terminal.

Isto ir√°:
1. Desplegar recurso Azure OpenAI com modelos GPT-5 e text-embedding-3-small
2. Gerar automaticamente o ficheiro `.env` na raiz do projeto com as credenciais
3. Configurar todas as vari√°veis de ambiente necess√°rias

**Est√° a ter problemas com o deployment?** Veja o [README da Infraestrutura](infra/README.md) para resolu√ß√£o detalhada de problemas incluindo conflitos de nomes de subdom√≠nios, passos manuais para deployment no Portal Azure e orienta√ß√£o sobre configura√ß√£o de modelos.

**Verifique se o deployment foi bem-sucedido:**

**Bash:**
```bash
cat ../.env  # Deve mostrar AZURE_OPENAI_ENDPOINT, API_KEY, etc.
```

**PowerShell:**
```powershell
Get-Content ..\.env  # Deve mostrar AZURE_OPENAI_ENDPOINT, API_KEY, etc.
```

> **Nota:** O comando `azd up` gera automaticamente o ficheiro `.env`. Se precisar de o atualizar mais tarde, pode editar o ficheiro `.env` manualmente ou regener√°-lo executando:
>
> **Bash:**
> ```bash
> cd ..
> bash .azd-env.sh
> ```
>
> **PowerShell:**
> ```powershell
> cd ..
> .\.azd-env.ps1
> ```

## Executar a Aplica√ß√£o Localmente

**Verifique o deployment:**

Assegure que o ficheiro `.env` existe na diretoria raiz com as credenciais Azure:

**Bash:**
```bash
cat ../.env  # Deve mostrar AZURE_OPENAI_ENDPOINT, API_KEY, DEPLOYMENT
```

**PowerShell:**
```powershell
Get-Content ..\.env  # Deve mostrar AZURE_OPENAI_ENDPOINT, API_KEY, DEPLOYMENT
```

**Inicie as aplica√ß√µes:**

**Op√ß√£o 1: Usar o Spring Boot Dashboard (Recomendado para utilizadores VS Code)**

O dev container inclui a extens√£o Spring Boot Dashboard, que fornece uma interface visual para gerir todas as aplica√ß√µes Spring Boot. Pode encontr√°-la na Barra de Atividades no lado esquerdo do VS Code (procure o √≠cone do Spring Boot).

No Spring Boot Dashboard, pode:
- Ver todas as aplica√ß√µes Spring Boot dispon√≠veis no workspace
- Iniciar/parar aplica√ß√µes com um clique
- Ver logs da aplica√ß√£o em tempo real
- Monitorizar o estado da aplica√ß√£o

Basta clicar no bot√£o de play ao lado de "introduction" para iniciar este m√≥dulo, ou iniciar todos os m√≥dulos de uma vez.

<img src="../../../translated_images/pt-PT/dashboard.69c7479aef09ff6b.webp" alt="Spring Boot Dashboard" width="400"/>

**Op√ß√£o 2: Usar scripts shell**

Inicie todas as aplica√ß√µes web (m√≥dulos 01-04):

**Bash:**
```bash
cd ..  # A partir do diret√≥rio raiz
./start-all.sh
```

**PowerShell:**
```powershell
cd ..  # A partir do diret√≥rio raiz
.\start-all.ps1
```

Ou inicie apenas este m√≥dulo:

**Bash:**
```bash
cd 01-introduction
./start.sh
```

**PowerShell:**
```powershell
cd 01-introduction
.\start.ps1
```

Ambos os scripts carregam automaticamente as vari√°veis de ambiente do ficheiro `.env` da raiz e ir√£o construir os JARs se n√£o existirem.

> **Nota:** Se preferir construir todos os m√≥dulos manualmente antes de iniciar:
>
> **Bash:**
> ```bash
> cd ..  # Go to root directory
> mvn clean package -DskipTests
> ```
>
> **PowerShell:**
> ```powershell
> cd ..  # Go to root directory
> mvn clean package -DskipTests
> ```

Abra http://localhost:8080 no seu navegador.

**Para parar:**

**Bash:**
```bash
./stop.sh  # Apenas este m√≥dulo
# Ou
cd .. && ./stop-all.sh  # Todos os m√≥dulos
```

**PowerShell:**
```powershell
.\stop.ps1  # Apenas este m√≥dulo
# Ou
cd ..; .\stop-all.ps1  # Todos os m√≥dulos
```

## Usar a Aplica√ß√£o

A aplica√ß√£o fornece uma interface web com duas implementa√ß√µes de chat lado a lado.

<img src="../../../translated_images/pt-PT/home-screen.121a03206ab910c0.webp" alt="Ecr√£ Inicial da Aplica√ß√£o" width="800"/>

*Dashboard mostrando as op√ß√µes de Chat Simples (sem estado) e Chat Conversacional (com estado)*

### Chat Sem Estado (Painel Esquerdo)

Experimente primeiro aqui. Pergunte "O meu nome √© Jo√£o" e depois imediatamente "Qual √© o meu nome?" O modelo n√£o vai lembrar porque cada mensagem √© independente. Isto demonstra o problema central da integra√ß√£o b√°sica de modelos de linguagem - sem contexto de conversa.

<img src="../../../translated_images/pt-PT/simple-chat-stateless-demo.13aeb3978eab3234.webp" alt="Demo de Chat Sem Estado" width="800"/>

*A IA n√£o lembra o seu nome da mensagem anterior*

### Chat Com Estado (Painel Direito)

Agora experimente a mesma sequ√™ncia aqui. Pergunte "O meu nome √© Jo√£o" e depois "Qual √© o meu nome?" Desta vez lembra-se. A diferen√ßa √© o MessageWindowChatMemory - mant√©m o hist√≥rico da conversa e inclui-o em cada pedido. √â assim que a IA conversacional de produ√ß√£o funciona.

<img src="../../../translated_images/pt-PT/conversational-chat-stateful-demo.e5be9822eb23ff59.webp" alt="Demo de Chat Com Estado" width="800"/>

*A IA lembra o seu nome de antes na conversa*

Ambos os pain√©is usam o mesmo modelo GPT-5. A √∫nica diferen√ßa √© a mem√≥ria. Isto torna claro o que a mem√≥ria traz para a sua aplica√ß√£o e porque √© essencial para casos de uso reais.

## Pr√≥ximos Passos

**Pr√≥ximo M√≥dulo:** [02-prompt-engineering - Engenharia de Prompt com GPT-5](../02-prompt-engineering/README.md)

---

**Navega√ß√£o:** [‚Üê Anterior: M√≥dulo 00 - In√≠cio R√°pido](../00-quick-start/README.md) | [Voltar ao Principal](../README.md) | [Seguinte: M√≥dulo 02 - Engenharia de Prompt ‚Üí](../02-prompt-engineering/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Aviso Legal**:
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, por favor tenha em conta que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorizada. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional humana. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes erradas decorrentes do uso desta tradu√ß√£o.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->