# Modul 00: Rask start

## Innholdsfortegnelse

- [Introduksjon](../../../00-quick-start)
- [Hva er LangChain4j?](../../../00-quick-start)
- [LangChain4j Avhengigheter](../../../00-quick-start)
- [Forutsetninger](../../../00-quick-start)
- [Oppsett](../../../00-quick-start)
  - [1. Skaff GitHub-tokenet ditt](../../../00-quick-start)
  - [2. Sett tokenet ditt](../../../00-quick-start)
- [Kj√∏r eksemplene](../../../00-quick-start)
  - [1. Grunnleggende chat](../../../00-quick-start)
  - [2. Prompt-m√∏nstre](../../../00-quick-start)
  - [3. Funksjonskalling](../../../00-quick-start)
  - [4. Dokument Q&A (RAG)](../../../00-quick-start)
  - [5. Ansvarlig AI](../../../00-quick-start)
- [Hva hvert eksempel viser](../../../00-quick-start)
- [Neste steg](../../../00-quick-start)
- [Feils√∏king](../../../00-quick-start)

## Introduksjon

Denne raskstarten er ment √• f√• deg i gang med LangChain4j s√• raskt som mulig. Den dekker det absolutte grunnlaget for √• bygge AI-applikasjoner med LangChain4j og GitHub Models. I de neste modulene vil du bruke Azure OpenAI med LangChain4j for √• bygge mer avanserte applikasjoner.

## Hva er LangChain4j?

LangChain4j er et Java-bibliotek som forenkler bygging av AI-drevne applikasjoner. I stedet for √• h√•ndtere HTTP-klienter og JSON-parsing, arbeider du med rene Java-APIer.

"Chain" i LangChain refererer til lenking av flere komponenter sammen ‚Äì du kan lenke en prompt til en modell til en parser, eller kjede flere AI-kall der ett output mates som neste input. Denne raskstarten fokuserer p√• grunnprinsippene f√∏r du utforsker mer komplekse kjeder.

<img src="../../../translated_images/no/langchain-concept.ad1fe6cf063515e1.webp" alt="LangChain4j Chaining Concept" width="800"/>

*Lenking av komponenter i LangChain4j ‚Äì byggeklosser kobles sammen for √• lage kraftige AI-arbeidsflyter*

Vi bruker tre kjernekomponenter:

**ChatLanguageModel** ‚Äì Grensesnittet for AI-modellinteraksjoner. Kall `model.chat("prompt")` og f√• tilbake en svarstreng. Vi bruker `OpenAiOfficialChatModel` som fungerer med OpenAI-kompatible endepunkter som GitHub Models.

**AiServices** ‚Äì Lager typesikre AI tjenestegrensesnitt. Definer metoder, annoter dem med `@Tool`, og LangChain4j h√•ndterer orkestreringen. AI kaller automatisk Java-metodene dine n√•r det trengs.

**MessageWindowChatMemory** ‚Äì Opprettholder samtalehistorikk. Uten dette er hver foresp√∏rsel uavhengig. Med det husker AI tidligere meldinger og opprettholder kontekst over flere turer.

<img src="../../../translated_images/no/architecture.eedc993a1c576839.webp" alt="LangChain4j Architecture" width="800"/>

*LangChain4j-arkitektur ‚Äì kjernekonponenter som samarbeider for √• drive AI-applikasjonene dine*

## LangChain4j Avhengigheter

Denne raskstarten bruker to Maven-avhengigheter i [`pom.xml`](../../../00-quick-start/pom.xml):

```xml
<!-- Core LangChain4j library -->
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>

<!-- OpenAI integration (works with GitHub Models) -->
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-open-ai-official</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
```

Modulen `langchain4j-open-ai-official` tilbyr klassen `OpenAiOfficialChatModel` som kobler til OpenAI-kompatible APIer. GitHub Models bruker samme API-format, s√• ingen spesiell adapter er n√∏dvendig ‚Äì bare pek base-URL til `https://models.github.ai/inference`.

## Forutsetninger

**Bruker du Dev Container?** Java og Maven er allerede installert. Du trenger bare et GitHub Personal Access Token.

**Lokal utvikling:**
- Java 21+, Maven 3.9+
- GitHub Personal Access Token (instruksjoner nedenfor)

> **Merk:** Denne modulen bruker `gpt-4.1-nano` fra GitHub Models. Ikke endre modellnavnet i koden ‚Äì det er konfigurert til √• fungere med GitHubs tilgjengelige modeller.

## Oppsett

### 1. Skaff GitHub-tokenet ditt

1. G√• til [GitHub Settings ‚Üí Personal Access Tokens](https://github.com/settings/personal-access-tokens)
2. Klikk "Generate new token"
3. Gi det et beskrivende navn (f.eks. "LangChain4j Demo")
4. Sett utl√∏pstid (7 dager anbefales)
5. Under "Account permissions", finn "Models" og sett til "Read-only"
6. Klikk "Generate token"
7. Kopier og lagre tokenet ditt ‚Äì du f√•r ikke se det igjen

### 2. Sett tokenet ditt

**Alternativ 1: Bruke VS Code (Anbefalt)**

Hvis du bruker VS Code, legg tokenet til i `.env`-filen i prosjektroten:

Hvis `.env`-filen ikke finnes, kopier `.env.example` til `.env` eller lag en ny `.env`-fil i prosjektroten.

**Eksempel p√• `.env`-fil:**
```bash
# I /workspaces/LangChain4j-for-Beginners/.env
GITHUB_TOKEN=your_token_here
```

Deretter kan du enkelt h√∏yreklikke p√• en hvilken som helst demo-fil (f.eks. `BasicChatDemo.java`) i Explorer og velge **"Run Java"** eller bruke startkonfigurasjonene i Kj√∏r og Feils√∏k-panelet.

**Alternativ 2: Bruke Terminal**

Sett tokenet som en milj√∏variabel:

**Bash:**
```bash
export GITHUB_TOKEN=your_token_here
```

**PowerShell:**
```powershell
$env:GITHUB_TOKEN=your_token_here
```

## Kj√∏r eksemplene

**Bruker VS Code:** H√∏yreklikk p√• en demo-fil i Explorer og velg **"Run Java"**, eller bruk startkonfigurasjonene i Kj√∏r og Feils√∏k-panelet (pass p√• at du har lagt til tokenet i `.env`-filen f√∏rst).

**Bruke Maven:** Alternativt kan du kj√∏re fra kommandolinjen:

### 1. Grunnleggende chat

**Bash:**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

**PowerShell:**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

### 2. Prompt-m√∏nstre

**Bash:**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.PromptEngineeringDemo
```

**PowerShell:**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.PromptEngineeringDemo
```

Viser zero-shot, few-shot, chain-of-thought, og rollebasert prompting.

### 3. Funksjonskalling

**Bash:**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.ToolIntegrationDemo
```

**PowerShell:**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.ToolIntegrationDemo
```

AI kaller automatisk Java-metodene dine n√•r det trengs.

### 4. Dokument Q&A (RAG)

**Bash:**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.SimpleReaderDemo
```

**PowerShell:**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.SimpleReaderDemo
```

Still sp√∏rsm√•l om innhold i `document.txt`.

### 5. Ansvarlig AI

**Bash:**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.ResponsibleAIDemo
```

**PowerShell:**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.ResponsibleAIDemo
```

Se hvordan AI-sikkerhetsfiltre blokkerer skadelig innhold.

## Hva hvert eksempel viser

**Grunnleggende chat** - [BasicChatDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/BasicChatDemo.java)

Start her for √• se LangChain4j p√• sitt enkleste. Du lager en `OpenAiOfficialChatModel`, sender en prompt med `.chat()` og f√•r tilbake et svar. Dette demonstrerer grunnlaget: hvordan man initialiserer modeller med tilpassede endepunkter og API-n√∏kler. N√•r du forst√•r dette m√∏nsteret, bygger alt annet videre p√• det.

```java
ChatLanguageModel model = OpenAiOfficialChatModel.builder()
    .baseUrl("https://models.github.ai/inference")
    .apiKey(System.getenv("GITHUB_TOKEN"))
    .modelName("gpt-4.1-nano")
    .build();

String response = model.chat("What is LangChain4j?");
System.out.println(response);
```

> **ü§ñ Pr√∏v med [GitHub Copilot](https://github.com/features/copilot) Chat:** √Öpne [`BasicChatDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/BasicChatDemo.java) og sp√∏r:
> - "Hvordan bytter jeg fra GitHub Models til Azure OpenAI i denne koden?"
> - "Hvilke andre parametere kan jeg konfigurere i OpenAiOfficialChatModel.builder()?"
> - "Hvordan legger jeg til str√∏mming av svar i stedet for √• vente p√• komplett svar?"

**Prompt Engineering** - [PromptEngineeringDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/PromptEngineeringDemo.java)

N√• som du vet hvordan du snakker til en modell, la oss utforske hva du sier til den. Denne demoen bruker samme modelloppsett, men viser fire forskjellige prompt-m√∏nstre. Pr√∏v zero-shot prompts for direkte instruksjoner, few-shot prompts som l√¶rer fra eksempler, chain-of-thought prompts som avsl√∏rer resonnementstrinn, og rollebaserte prompts som setter kontekst. Du vil se hvordan samme modell gir dramatisk forskjellige resultater basert p√• hvordan du rammer inn foresp√∏rselen.

```java
PromptTemplate template = PromptTemplate.from(
    "What's the best time to visit {{destination}} for {{activity}}?"
);

Prompt prompt = template.apply(Map.of(
    "destination", "Paris",
    "activity", "sightseeing"
));

String response = model.chat(prompt.text());
```

> **ü§ñ Pr√∏v med [GitHub Copilot](https://github.com/features/copilot) Chat:** √Öpne [`PromptEngineeringDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/PromptEngineeringDemo.java) og sp√∏r:
> - "Hva er forskjellen p√• zero-shot og few-shot prompting, og n√•r b√∏r jeg bruke hver av dem?"
> - "Hvordan p√•virker temperaturparameteren modellens svar?"
> - "Hva er noen teknikker for √• forhindre prompt-injeksjonsangrep i produksjon?"
> - "Hvordan kan jeg lage gjenbrukbare PromptTemplate-objekter for vanlige m√∏nstre?"

**Tool Integration** - [ToolIntegrationDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/ToolIntegrationDemo.java)

Her blir LangChain4j kraftig. Du bruker `AiServices` for √• lage en AI-assistent som kan kalle Java-metodene dine. Bare annoter metoder med `@Tool("beskrivelse")` og LangChain4j tar seg av resten ‚Äì AI bestemmer automatisk n√•r den skal bruke hvert verkt√∏y basert p√• hva brukeren sp√∏r om. Dette demonstrerer funksjonskalling, en viktig teknikk for √• bygge AI som kan utf√∏re handlinger, ikke bare svare p√• sp√∏rsm√•l.

```java
@Tool("Performs addition of two numeric values")
public double add(double a, double b) {
    return a + b;
}

MathAssistant assistant = AiServices.create(MathAssistant.class, model);
String response = assistant.chat("What is 25 plus 17?");
```

> **ü§ñ Pr√∏v med [GitHub Copilot](https://github.com/features/copilot) Chat:** √Öpne [`ToolIntegrationDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/ToolIntegrationDemo.java) og sp√∏r:
> - "Hvordan fungerer @Tool-annotasjonen og hva gj√∏r LangChain4j med den bak kulissene?"
> - "Kan AI kalle flere verkt√∏y i rekkef√∏lge for √• l√∏se komplekse problemer?"
> - "Hva skjer hvis et verkt√∏y kaster en unntak ‚Äì hvordan b√∏r jeg h√•ndtere feil?"
> - "Hvordan ville jeg integrert et ekte API i stedet for dette kalkulator-eksemplet?"

**Dokument Q&A (RAG)** - [SimpleReaderDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/SimpleReaderDemo.java)

Her ser du grunnlaget for RAG (retrieval-augmented generation). I stedet for √• stole p√• modellens treningsdata, laster du inn innhold fra [`document.txt`](../../../00-quick-start/document.txt) og inkluderer det i prompten. AI svarer basert p√• dokumentet ditt, ikke p√• sin generelle kunnskap. Dette er f√∏rste steg mot √• bygge systemer som kan jobbe med dine egne data.

```java
Document document = FileSystemDocumentLoader.loadDocument("document.txt");
String content = document.text();

String prompt = "Based on this document: " + content + 
                "\nQuestion: What is the main topic?";
String response = model.chat(prompt);
```

> **Merk:** Denne enkle tiln√¶rmingen laster hele dokumentet inn i prompten. For store filer (>10KB) vil du overskride kontekstgrenser. Modul 03 dekker oppdeling og vektors√∏k for produksjons-RAG-systemer.

> **ü§ñ Pr√∏v med [GitHub Copilot](https://github.com/features/copilot) Chat:** √Öpne [`SimpleReaderDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/SimpleReaderDemo.java) og sp√∏r:
> - "Hvordan hindrer RAG AI-hallusinasjoner sammenlignet med bruk av modellens treningsdata?"
> - "Hva er forskjellen p√• denne enkle tiln√¶rmingen og bruk av vektorrepresentasjoner for gjenfinning?"
> - "Hvordan ville jeg skalert dette til √• h√•ndtere flere dokumenter eller st√∏rre kunnskapsbaser?"
> - "Hva er beste praksis for √• strukturere prompten for √• sikre at AI kun bruker den gitte konteksten?"

**Ansvarlig AI** - [ResponsibleAIDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/ResponsibleAIDemo.java)

Bygg AI-sikkerhet med forsvar i dybden. Denne demoen viser to lag med beskyttelse som jobber sammen:

**Del 1: LangChain4j Input Guardrails** ‚Äì Blokker farlige prompts f√∏r de n√•r LLM. Lag tilpassede "guardrails" som sjekker etter forbudte n√∏kkelord eller m√∏nstre. Disse kj√∏res i koden din, s√• de er raske og gratis.

```java
class DangerousContentGuardrail implements InputGuardrail {
    @Override
    public InputGuardrailResult validate(UserMessage userMessage) {
        String text = userMessage.singleText().toLowerCase();
        if (text.contains("explosives")) {
            return fatal("Blocked: contains prohibited keyword");
        }
        return success();
    }
}
```

**Del 2: Provider Safety Filters** ‚Äì GitHub Models har innebygde filtre som fanger opp det dine guardrails kan mangle. Du vil se harde blokker (HTTP 400-feil) for alvorlige brudd og myke avslag der AI h√∏flig avsl√•r.

> **ü§ñ Pr√∏v med [GitHub Copilot](https://github.com/features/copilot) Chat:** √Öpne [`ResponsibleAIDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/ResponsibleAIDemo.java) og sp√∏r:
> - "Hva er InputGuardrail og hvordan lager jeg mine egne?"
> - "Hva er forskjellen p√• en hard blokk og et mykt avslag?"
> - "Hvorfor bruke b√•de guardrails og providers filtre sammen?"

## Neste steg

**Neste modul:** [01-introduction - Komme i gang med LangChain4j og gpt-5 p√• Azure](../01-introduction/README.md)

---

**Navigasjon:** [‚Üê Tilbake til hovedmeny](../README.md) | [Neste: Modul 01 - Introduksjon ‚Üí](../01-introduction/README.md)

---

## Feils√∏king

### F√∏rste Maven-build

**Problem:** F√∏rste `mvn clean compile` eller `mvn package` tar lang tid (10-15 minutter)

**√Örsak:** Maven m√• laste ned alle prosjektavhengigheter (Spring Boot, LangChain4j-biblioteker, Azure SDKer osv.) ved f√∏rste bygging.

**L√∏sning:** Dette er normal oppf√∏rsel. Etterf√∏lgende bygginger g√•r mye raskere fordi avhengigheter caches lokalt. Nedlastningstid avhenger av nettverkshastigheten din.

### PowerShell Maven-kommando syntaks

**Problem:** Maven-kommandoer feiler med feilmelding `Unknown lifecycle phase ".mainClass=..."`

**√Örsak:** PowerShell tolker `=` som en variabeltilordningsoperator, noe som bryter Maven-eiendomssyntaksen.
**L√∏sning**: Bruk stopp-parsing-operatoren `--%` f√∏r Maven-kommandoen:

**PowerShell:**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

**Bash:**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

`--%`-operatoren forteller PowerShell √• sende alle resterende argumenter bokstavelig til Maven uten tolkning.

### Windows PowerShell Emoji-visning

**Problem**: AI-svar viser s√∏ppeltegn (f.eks. `????` eller `√¢??`) i stedet for emojis i PowerShell

**√Örsak**: PowerShells standardkoding st√∏tter ikke UTF-8-emojis

**L√∏sning**: Kj√∏r denne kommandoen f√∏r du kj√∏rer Java-applikasjoner:
```cmd
chcp 65001
```

Dette tvinger UTF-8-koding i terminalen. Alternativt bruk Windows Terminal som har bedre Unicode-st√∏tte.

### Feils√∏king av API-kall

**Problem**: Autentiseringsfeil, grenseverdier eller uventede svar fra AI-modellen

**L√∏sning**: Eksemplene inkluderer `.logRequests(true)` og `.logResponses(true)` for √• vise API-kall i konsollen. Dette hjelper med √• feils√∏ke autentiseringsfeil, grenseverdier eller uventede svar. Fjern disse flaggene i produksjon for √• redusere loggmengden.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfraskrivelse**:  
Dette dokumentet har blitt oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, vennligst v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det opprinnelige dokumentet p√• dets originalsproget b√∏r betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->