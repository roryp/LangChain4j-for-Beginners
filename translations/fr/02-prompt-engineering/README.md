<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8d787826cad7e92bf5cdbd116b1e6116",
  "translation_date": "2025-12-13T15:45:30+00:00",
  "source_file": "02-prompt-engineering/README.md",
  "language_code": "fr"
}
-->
# Module 02 : Ing√©nierie des invites avec GPT-5

## Table des mati√®res

- [Ce que vous apprendrez](../../../02-prompt-engineering)
- [Pr√©requis](../../../02-prompt-engineering)
- [Comprendre l'ing√©nierie des invites](../../../02-prompt-engineering)
- [Comment cela utilise LangChain4j](../../../02-prompt-engineering)
- [Les mod√®les principaux](../../../02-prompt-engineering)
- [Utilisation des ressources Azure existantes](../../../02-prompt-engineering)
- [Captures d'√©cran de l'application](../../../02-prompt-engineering)
- [Explorer les mod√®les](../../../02-prompt-engineering)
  - [Faible vs forte impatience](../../../02-prompt-engineering)
  - [Ex√©cution de t√¢ches (pr√©ambules d'outils)](../../../02-prompt-engineering)
  - [Code auto-r√©fl√©chissant](../../../02-prompt-engineering)
  - [Analyse structur√©e](../../../02-prompt-engineering)
  - [Chat multi-tours](../../../02-prompt-engineering)
  - [Raisonnement √©tape par √©tape](../../../02-prompt-engineering)
  - [Sortie contrainte](../../../02-prompt-engineering)
- [Ce que vous apprenez vraiment](../../../02-prompt-engineering)
- [√âtapes suivantes](../../../02-prompt-engineering)

## Ce que vous apprendrez

Dans le module pr√©c√©dent, vous avez vu comment la m√©moire permet l'IA conversationnelle et utilis√© les mod√®les GitHub pour des interactions basiques. Maintenant, nous allons nous concentrer sur la mani√®re de poser des questions - les invites elles-m√™mes - en utilisant GPT-5 d'Azure OpenAI. La fa√ßon dont vous structurez vos invites affecte consid√©rablement la qualit√© des r√©ponses que vous obtenez.

Nous utiliserons GPT-5 car il introduit un contr√¥le du raisonnement - vous pouvez indiquer au mod√®le combien de r√©flexion faire avant de r√©pondre. Cela rend les diff√©rentes strat√©gies d'invite plus apparentes et vous aide √† comprendre quand utiliser chaque approche. Nous b√©n√©ficierons √©galement des limites de taux moins strictes d'Azure pour GPT-5 compar√© aux mod√®les GitHub.

## Pr√©requis

- Module 01 termin√© (ressources Azure OpenAI d√©ploy√©es)
- Fichier `.env` dans le r√©pertoire racine avec les identifiants Azure (cr√©√© par `azd up` dans le Module 01)

> **Note :** Si vous n'avez pas termin√© le Module 01, suivez d'abord les instructions de d√©ploiement l√†-bas.

## Comprendre l'ing√©nierie des invites

L'ing√©nierie des invites consiste √† concevoir un texte d'entr√©e qui vous donne syst√©matiquement les r√©sultats dont vous avez besoin. Ce n'est pas seulement poser des questions - c'est structurer les requ√™tes pour que le mod√®le comprenne exactement ce que vous voulez et comment le fournir.

Pensez-y comme donner des instructions √† un coll√®gue. "Corrige le bug" est vague. "Corrige l'exception de pointeur nul dans UserService.java ligne 45 en ajoutant une v√©rification de null" est pr√©cis. Les mod√®les de langage fonctionnent de la m√™me mani√®re - la sp√©cificit√© et la structure comptent.

## Comment cela utilise LangChain4j

Ce module d√©montre des mod√®les avanc√©s d'invites en utilisant la m√™me base LangChain4j des modules pr√©c√©dents, avec un focus sur la structure des invites et le contr√¥le du raisonnement.

<img src="../../../translated_images/langchain4j-flow.48e534666213010bf4aab0e5e24a291c193a29cc80bf1864809b10d8ef2e9da2.fr.png" alt="LangChain4j Flow" width="800"/>

*Comment LangChain4j connecte vos invites √† Azure OpenAI GPT-5*

**D√©pendances** - Le Module 02 utilise les d√©pendances langchain4j suivantes d√©finies dans `pom.xml` :
```xml
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-open-ai-official</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
```

**Configuration OpenAiOfficialChatModel** - [LangChainConfig.java](../../../02-prompt-engineering/src/main/java/com/example/langchain4j/prompts/config/LangChainConfig.java)

Le mod√®le de chat est configur√© manuellement comme un bean Spring en utilisant le client officiel OpenAI, qui supporte les points de terminaison Azure OpenAI. La diff√©rence cl√© avec le Module 01 est la fa√ßon dont nous structurons les invites envoy√©es √† `chatModel.chat()`, pas la configuration du mod√®le lui-m√™me.

**Messages syst√®me et utilisateur** - [Gpt5PromptService.java](../../../02-prompt-engineering/src/main/java/com/example/langchain4j/prompts/service/Gpt5PromptService.java)

LangChain4j s√©pare les types de messages pour plus de clart√©. `SystemMessage` d√©finit le comportement et le contexte de l'IA (comme "Vous √™tes un relecteur de code"), tandis que `UserMessage` contient la requ√™te r√©elle. Cette s√©paration vous permet de maintenir un comportement IA coh√©rent √† travers diff√©rentes requ√™tes utilisateur.

```java
SystemMessage systemMsg = SystemMessage.from(
    "You are a helpful Java programming expert."
);

UserMessage userMsg = UserMessage.from(
    "Explain what a List is in Java"
);

String response = chatModel.chat(systemMsg, userMsg);
```

<img src="../../../translated_images/message-types.93e0779798a17c9d4c89aebee57aac31454a8980a8f22e92f73b034ea7806484.fr.png" alt="Message Types Architecture" width="800"/>

*SystemMessage fournit un contexte persistant tandis que UserMessages contiennent les requ√™tes individuelles*

**MessageWindowChatMemory pour multi-tours** - Pour le mod√®le de conversation multi-tours, nous r√©utilisons `MessageWindowChatMemory` du Module 01. Chaque session obtient sa propre instance de m√©moire stock√©e dans une `Map<String, ChatMemory>`, permettant plusieurs conversations simultan√©es sans m√©lange de contexte.

**Mod√®les d'invites** - Le vrai focus ici est l'ing√©nierie des invites, pas de nouvelles API LangChain4j. Chaque mod√®le (faible impatience, forte impatience, ex√©cution de t√¢ches, etc.) utilise la m√™me m√©thode `chatModel.chat(prompt)` mais avec des cha√Ænes d'invite soigneusement structur√©es. Les balises XML, instructions et formatages font tous partie du texte de l'invite, pas des fonctionnalit√©s LangChain4j.

**Contr√¥le du raisonnement** - L'effort de raisonnement de GPT-5 est contr√¥l√© via des instructions dans l'invite comme "maximum 2 √©tapes de raisonnement" ou "explorez en profondeur". Ce sont des techniques d'ing√©nierie des invites, pas des configurations LangChain4j. La biblioth√®que se contente de transmettre vos invites au mod√®le.

Le point cl√© : LangChain4j fournit l'infrastructure (connexion au mod√®le via [LangChainConfig.java](../../../02-prompt-engineering/src/main/java/com/example/langchain4j/prompts/config/LangChainConfig.java), m√©moire, gestion des messages via [Gpt5PromptService.java](../../../02-prompt-engineering/src/main/java/com/example/langchain4j/prompts/service/Gpt5PromptService.java)), tandis que ce module vous apprend √† cr√©er des invites efficaces dans cette infrastructure.

## Les mod√®les principaux

Tous les probl√®mes ne n√©cessitent pas la m√™me approche. Certaines questions demandent des r√©ponses rapides, d'autres une r√©flexion approfondie. Certaines n√©cessitent un raisonnement visible, d'autres juste des r√©sultats. Ce module couvre huit mod√®les d'invites - chacun optimis√© pour diff√©rents sc√©narios. Vous exp√©rimenterez tous pour apprendre quand chaque approche fonctionne le mieux.

<img src="../../../translated_images/eight-patterns.fa1ebfdf16f71e9a7629ee0801892bdf9a91ab03ace969c925310d9ba1b5d2e5.fr.png" alt="Eight Prompting Patterns" width="800"/>

*Vue d'ensemble des huit mod√®les d'ing√©nierie des invites et leurs cas d'utilisation*

<img src="../../../translated_images/reasoning-effort.db4a3ba5b8e392c1835f8ec00d5d832a665be5adc1a2f3c08f46edfcfb385242.fr.png" alt="Reasoning Effort Comparison" width="800"/>

*Faible impatience (rapide, direct) vs forte impatience (approfondi, exploratoire) dans les approches de raisonnement*

**Faible impatience (rapide & cibl√©)** - Pour les questions simples o√π vous voulez des r√©ponses rapides et directes. Le mod√®le fait un raisonnement minimal - maximum 2 √©tapes. Utilisez ceci pour des calculs, recherches ou questions simples.

```java
String prompt = """
    <reasoning_effort>low</reasoning_effort>
    <instruction>maximum 2 reasoning steps</instruction>
    
    What is 15% of 200?
    """;

String response = chatModel.chat(prompt);
```

> üí° **Explorez avec GitHub Copilot :** Ouvrez [`Gpt5PromptService.java`](../../../02-prompt-engineering/src/main/java/com/example/langchain4j/prompts/service/Gpt5PromptService.java) et demandez :
> - "Quelle est la diff√©rence entre les mod√®les d'invite √† faible impatience et forte impatience ?"
> - "Comment les balises XML dans les invites aident-elles √† structurer la r√©ponse de l'IA ?"
> - "Quand devrais-je utiliser les mod√®les d'auto-r√©flexion vs les instructions directes ?"

**Forte impatience (profond & approfondi)** - Pour les probl√®mes complexes o√π vous voulez une analyse compl√®te. Le mod√®le explore en profondeur et montre un raisonnement d√©taill√©. Utilisez ceci pour la conception syst√®me, d√©cisions d'architecture ou recherches complexes.

```java
String prompt = """
    <reasoning_effort>high</reasoning_effort>
    <instruction>explore thoroughly, show detailed reasoning</instruction>
    
    Design a caching strategy for a high-traffic REST API.
    """;

String response = chatModel.chat(prompt);
```

**Ex√©cution de t√¢ches (progression √©tape par √©tape)** - Pour les flux de travail en plusieurs √©tapes. Le mod√®le fournit un plan initial, narre chaque √©tape au fur et √† mesure, puis donne un r√©sum√©. Utilisez ceci pour les migrations, impl√©mentations ou tout processus multi-√©tapes.

```java
String prompt = """
    <task>Create a REST endpoint for user registration</task>
    <preamble>Provide an upfront plan</preamble>
    <narration>Narrate each step as you work</narration>
    <summary>Summarize what was accomplished</summary>
    """;

String response = chatModel.chat(prompt);
```

Le prompt Chain-of-Thought demande explicitement au mod√®le de montrer son processus de raisonnement, am√©liorant la pr√©cision pour les t√¢ches complexes. La d√©composition √©tape par √©tape aide humains et IA √† comprendre la logique.

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Demandez √† propos de ce mod√®le :
> - "Comment adapterais-je le mod√®le d'ex√©cution de t√¢ches pour des op√©rations longues ?"
> - "Quelles sont les meilleures pratiques pour structurer les pr√©ambules d'outils dans des applications en production ?"
> - "Comment capturer et afficher les mises √† jour de progression interm√©diaires dans une interface utilisateur ?"

<img src="../../../translated_images/task-execution-pattern.9da3967750ab5c1e74ee149d54795d096fa4485540b49c07dcfec48e19d6a7e0.fr.png" alt="Task Execution Pattern" width="800"/>

*Planifier ‚Üí Ex√©cuter ‚Üí R√©sumer le flux de travail pour les t√¢ches multi-√©tapes*

**Code auto-r√©fl√©chissant** - Pour g√©n√©rer du code de qualit√© production. Le mod√®le g√©n√®re du code, le v√©rifie selon des crit√®res de qualit√©, et l'am√©liore it√©rativement. Utilisez ceci lors de la cr√©ation de nouvelles fonctionnalit√©s ou services.

```java
String prompt = """
    <task>Create an email validation service</task>
    <quality_criteria>
    - Correct logic and error handling
    - Best practices (clean code, proper naming)
    - Performance optimization
    - Security considerations
    </quality_criteria>
    <instruction>Generate code, evaluate against criteria, improve iteratively</instruction>
    """;

String response = chatModel.chat(prompt);
```

<img src="../../../translated_images/self-reflection-cycle.6f71101ca0bd28cc9f4ea2d0212a18774750ae8684639e670f0870a1dfc750d8.fr.png" alt="Self-Reflection Cycle" width="800"/>

*Boucle d'am√©lioration it√©rative - g√©n√©rer, √©valuer, identifier les probl√®mes, am√©liorer, r√©p√©ter*

**Analyse structur√©e** - Pour une √©valuation coh√©rente. Le mod√®le passe en revue le code en utilisant un cadre fixe (correction, pratiques, performance, s√©curit√©). Utilisez ceci pour les revues de code ou √©valuations qualit√©.

```java
String prompt = """
    <code>
    public List getUsers() {
        return database.query("SELECT * FROM users");
    }
    </code>
    
    <framework>
    Evaluate using these categories:
    1. Correctness - Logic and functionality
    2. Best Practices - Code quality
    3. Performance - Efficiency concerns
    4. Security - Vulnerabilities
    </framework>
    """;

String response = chatModel.chat(prompt);
```

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Demandez √† propos de l'analyse structur√©e :
> - "Comment personnaliser le cadre d'analyse pour diff√©rents types de revues de code ?"
> - "Quelle est la meilleure fa√ßon d'analyser et d'agir sur une sortie structur√©e de mani√®re programmatique ?"
> - "Comment assurer des niveaux de gravit√© coh√©rents √† travers diff√©rentes sessions de revue ?"

<img src="../../../translated_images/structured-analysis-pattern.0af3b690b60cf2d655ebaa49aa6450d9f3c1ff41d69bdf628c9b44cdc0e25ad1.fr.png" alt="Structured Analysis Pattern" width="800"/>

*Cadre √† quatre cat√©gories pour des revues de code coh√©rentes avec niveaux de gravit√©*

**Chat multi-tours** - Pour les conversations n√©cessitant du contexte. Le mod√®le se souvient des messages pr√©c√©dents et construit dessus. Utilisez ceci pour des sessions d'aide interactives ou Q&A complexes.

```java
ChatMemory memory = MessageWindowChatMemory.withMaxMessages(10);

memory.add(UserMessage.from("What is Spring Boot?"));
AiMessage aiMessage1 = chatModel.chat(memory.messages()).aiMessage();
memory.add(aiMessage1);

memory.add(UserMessage.from("Show me an example"));
AiMessage aiMessage2 = chatModel.chat(memory.messages()).aiMessage();
memory.add(aiMessage2);
```

<img src="../../../translated_images/context-memory.dff30ad9fa78832afd78482b6d21f5488e710d99412f89747977c37c4269b559.fr.png" alt="Context Memory" width="800"/>

*Comment le contexte de la conversation s'accumule sur plusieurs tours jusqu'√† atteindre la limite de tokens*

**Raisonnement √©tape par √©tape** - Pour les probl√®mes n√©cessitant une logique visible. Le mod√®le montre un raisonnement explicite pour chaque √©tape. Utilisez ceci pour des probl√®mes math√©matiques, puzzles logiques, ou quand vous devez comprendre le processus de pens√©e.

```java
String prompt = """
    <instruction>Show your reasoning step-by-step</instruction>
    
    If a train travels 120 km in 2 hours, then stops for 30 minutes,
    then travels another 90 km in 1.5 hours, what is the average speed
    for the entire journey including the stop?
    """;

String response = chatModel.chat(prompt);
```

<img src="../../../translated_images/step-by-step-pattern.a99ea4ca1c48578c9dbe39c75ce10a80cd4eafd6293c84cb7c77d66da9d10fba.fr.png" alt="Step-by-Step Pattern" width="800"/>

*D√©composer les probl√®mes en √©tapes logiques explicites*

**Sortie contrainte** - Pour des r√©ponses avec des exigences sp√©cifiques de format. Le mod√®le suit strictement les r√®gles de format et de longueur. Utilisez ceci pour des r√©sum√©s ou quand vous avez besoin d'une structure de sortie pr√©cise.

```java
String prompt = """
    <constraints>
    - Exactly 100 words
    - Bullet point format
    - Technical terms only
    </constraints>
    
    Summarize the key concepts of machine learning.
    """;

String response = chatModel.chat(prompt);
```

<img src="../../../translated_images/constrained-output-pattern.0ce39a682a6795c219d5574bb05cac9f398937c25765b1715c95231c2efd0a17.fr.png" alt="Constrained Output Pattern" width="800"/>

*Imposition de r√®gles sp√©cifiques de format, longueur et structure*

## Utilisation des ressources Azure existantes

**V√©rifier le d√©ploiement :**

Assurez-vous que le fichier `.env` existe dans le r√©pertoire racine avec les identifiants Azure (cr√©√© lors du Module 01) :
```bash
cat ../.env  # Devrait afficher AZURE_OPENAI_ENDPOINT, API_KEY, DEPLOYMENT
```

**D√©marrer l'application :**

> **Note :** Si vous avez d√©j√† d√©marr√© toutes les applications avec `./start-all.sh` du Module 01, ce module est d√©j√† en cours d'ex√©cution sur le port 8083. Vous pouvez ignorer les commandes de d√©marrage ci-dessous et aller directement sur http://localhost:8083.

**Option 1 : Utiliser le Spring Boot Dashboard (recommand√© pour les utilisateurs VS Code)**

Le conteneur de d√©veloppement inclut l'extension Spring Boot Dashboard, qui fournit une interface visuelle pour g√©rer toutes les applications Spring Boot. Vous la trouverez dans la barre d'activit√© √† gauche de VS Code (cherchez l'ic√¥ne Spring Boot).

Depuis le Spring Boot Dashboard, vous pouvez :
- Voir toutes les applications Spring Boot disponibles dans l'espace de travail
- D√©marrer/arr√™ter les applications en un clic
- Voir les logs des applications en temps r√©el
- Surveiller l'√©tat des applications

Cliquez simplement sur le bouton lecture √† c√¥t√© de "prompt-engineering" pour d√©marrer ce module, ou d√©marrez tous les modules en m√™me temps.

<img src="../../../translated_images/dashboard.da2c2130c904aaf0369545a63f4b54003ff3c08cbde55e8d66bd99a238eda541.fr.png" alt="Spring Boot Dashboard" width="400"/>

**Option 2 : Utiliser les scripts shell**

D√©marrer toutes les applications web (modules 01-04) :

**Bash :**
```bash
cd ..  # Depuis le r√©pertoire racine
./start-all.sh
```

**PowerShell :**
```powershell
cd ..  # Depuis le r√©pertoire racine
.\start-all.ps1
```

Ou d√©marrer uniquement ce module :

**Bash :**
```bash
cd 02-prompt-engineering
./start.sh
```

**PowerShell :**
```powershell
cd 02-prompt-engineering
.\start.ps1
```

Les deux scripts chargent automatiquement les variables d'environnement depuis le fichier `.env` racine et construiront les JARs s'ils n'existent pas.

> **Note :** Si vous pr√©f√©rez construire manuellement tous les modules avant de d√©marrer :
>
> **Bash :**
> ```bash
> cd ..  # Go to root directory
> mvn clean package -DskipTests
> ```
>
> **PowerShell :**
> ```powershell
> cd ..  # Go to root directory
> mvn clean package -DskipTests
> ```

Ouvrez http://localhost:8083 dans votre navigateur.

**Pour arr√™ter :**

**Bash :**
```bash
./stop.sh  # Ce module uniquement
# Ou
cd .. && ./stop-all.sh  # Tous les modules
```

**PowerShell :**
```powershell
.\stop.ps1  # Ce module uniquement
# Ou
cd ..; .\stop-all.ps1  # Tous les modules
```

## Captures d'√©cran de l'application

<img src="../../../translated_images/dashboard-home.5444dbda4bc1f79d0bdcf43a4faa19a14066ddb88910a3128e9817018c0fefea.fr.png" alt="Dashboard Home" width="800" style="border: 1px solid #ddd; box-shadow: 0 2px 8px rgba(0,0,0,0.1);"/>

*Le tableau de bord principal montrant les 8 mod√®les d'ing√©nierie des invites avec leurs caract√©ristiques et cas d'utilisation*

## Explorer les mod√®les

L'interface web vous permet d'exp√©rimenter diff√©rentes strat√©gies d'invite. Chaque mod√®le r√©sout des probl√®mes diff√©rents - essayez-les pour voir quand chaque approche est la plus efficace.

### Faible vs forte impatience

Posez une question simple comme "Quel est 15 % de 200 ?" en utilisant Faible impatience. Vous obtiendrez une r√©ponse instantan√©e et directe. Maintenant posez quelque chose de complexe comme "Concevez une strat√©gie de mise en cache pour une API √† fort trafic" en utilisant Forte impatience. Regardez comment le mod√®le ralentit et fournit un raisonnement d√©taill√©. M√™me mod√®le, m√™me structure de question - mais l'invite lui dit combien r√©fl√©chir.

<img src="../../../translated_images/low-eagerness-demo.898894591fb23aa0566a7f7e5315362621bf1c5794a724f0ae3d541f4fa8dc2c.fr.png" alt="Low Eagerness Demo" width="800"/>
*Calcul rapide avec un raisonnement minimal*

<img src="../../../translated_images/high-eagerness-demo.4ac93e7786c5a3768855b2891017f3880fe8ad0008044a6887a1f0665d2b45b7.fr.png" alt="D√©monstration de haute motivation" width="800"/>

*Strat√©gie de mise en cache compl√®te (2,8 Mo)*

### Ex√©cution de t√¢che (Pr√©ambules d'outil)

Les workflows √† √©tapes multiples b√©n√©ficient d'une planification pr√©alable et d'une narration des progr√®s. Le mod√®le d√©crit ce qu'il va faire, narre chaque √©tape, puis r√©sume les r√©sultats.

<img src="../../../translated_images/tool-preambles-demo.3ca4881e417f2e28505e2e9c6d84639dd655719a1aab08afd393e9ace0ba26c9.fr.png" alt="D√©monstration d'ex√©cution de t√¢che" width="800"/>

*Cr√©ation d'un point de terminaison REST avec narration √©tape par √©tape (3,9 Mo)*

### Code auto-r√©fl√©chi

Essayez "Cr√©er un service de validation d'email". Au lieu de simplement g√©n√©rer du code et s'arr√™ter, le mod√®le g√©n√®re, √©value selon des crit√®res de qualit√©, identifie les faiblesses, et am√©liore. Vous le verrez it√©rer jusqu'√† ce que le code atteigne les standards de production.

<img src="../../../translated_images/self-reflecting-code-demo.851ee05c988e743fdd7515224b9f9ffc4f579c31eb6f120254ee0adbb9637c75.fr.png" alt="D√©monstration de code auto-r√©fl√©chi" width="800"/>

*Service complet de validation d'email (5,2 Mo)*

### Analyse structur√©e

Les revues de code n√©cessitent des cadres d'√©valuation coh√©rents. Le mod√®le analyse le code en utilisant des cat√©gories fixes (exactitude, pratiques, performance, s√©curit√©) avec des niveaux de gravit√©.

<img src="../../../translated_images/structured-analysis-demo.9ef892194cd23bc889f7a2f903563b08196cfbaadcc9b3622a2aad0920f0f533.fr.png" alt="D√©monstration d'analyse structur√©e" width="800"/>

*Revue de code bas√©e sur un cadre*

### Chat multi-tours

Demandez "Qu'est-ce que Spring Boot ?" puis imm√©diatement "Montre-moi un exemple". Le mod√®le se souvient de votre premi√®re question et vous donne un exemple sp√©cifique de Spring Boot. Sans m√©moire, cette deuxi√®me question serait trop vague.

<img src="../../../translated_images/multi-turn-chat-demo.0d2d9b9a86a12b4b9859a48b7706d6a0504312318d6b4aba11a8fa70f4433ced.fr.png" alt="D√©monstration de chat multi-tours" width="800"/>

*Pr√©servation du contexte entre les questions*

### Raisonnement √©tape par √©tape

Choisissez un probl√®me math√©matique et essayez-le avec Raisonnement √©tape par √©tape et Faible motivation. La faible motivation vous donne juste la r√©ponse - rapide mais opaque. Le raisonnement √©tape par √©tape vous montre chaque calcul et d√©cision.

<img src="../../../translated_images/step-by-step-reasoning-demo.12139513356faecd76b77f132b914f8ea88557ac6a9a4221814b996221a1f3d4.fr.png" alt="D√©monstration de raisonnement √©tape par √©tape" width="800"/>

*Probl√®me math√©matique avec √©tapes explicites*

### Sortie contrainte

Quand vous avez besoin de formats sp√©cifiques ou de nombre de mots, ce mod√®le impose une stricte conformit√©. Essayez de g√©n√©rer un r√©sum√© avec exactement 100 mots en format liste √† puces.

<img src="../../../translated_images/constrained-output-demo.567cc45b75da16331c010346aead3e7554fdc4a933ad27854909b35130ac3fc5.fr.png" alt="D√©monstration de sortie contrainte" width="800"/>

*R√©sum√© d'apprentissage automatique avec contr√¥le du format*

## Ce que vous apprenez vraiment

**L'effort de raisonnement change tout**

GPT-5 vous permet de contr√¥ler l'effort computationnel via vos invites. Un faible effort signifie des r√©ponses rapides avec une exploration minimale. Un effort √©lev√© signifie que le mod√®le prend le temps de r√©fl√©chir profond√©ment. Vous apprenez √† adapter l'effort √† la complexit√© de la t√¢che - ne perdez pas de temps sur des questions simples, mais ne pr√©cipitez pas non plus les d√©cisions complexes.

**La structure guide le comportement**

Vous remarquez les balises XML dans les invites ? Elles ne sont pas d√©coratives. Les mod√®les suivent des instructions structur√©es plus fiablement que du texte libre. Quand vous avez besoin de processus √† √©tapes multiples ou de logique complexe, la structure aide le mod√®le √† suivre o√π il en est et ce qui vient ensuite.

<img src="../../../translated_images/prompt-structure.a77763d63f4e2f899e0c34d4b714d92d7cc50618c510310567b5885ce82f4a94.fr.png" alt="Structure de l'invite" width="800"/>

*Anatomie d'une invite bien structur√©e avec sections claires et organisation de style XML*

**Qualit√© par auto-√©valuation**

Les mod√®les auto-r√©fl√©chissants fonctionnent en rendant explicites les crit√®res de qualit√©. Au lieu d'esp√©rer que le mod√®le "fasse bien", vous lui dites exactement ce que signifie "bien" : logique correcte, gestion des erreurs, performance, s√©curit√©. Le mod√®le peut alors √©valuer sa propre sortie et s'am√©liorer. Cela transforme la g√©n√©ration de code d'une loterie en un processus.

**Le contexte est fini**

Les conversations multi-tours fonctionnent en incluant l'historique des messages √† chaque requ√™te. Mais il y a une limite - chaque mod√®le a un nombre maximal de tokens. √Ä mesure que les conversations grandissent, vous aurez besoin de strat√©gies pour garder le contexte pertinent sans atteindre ce plafond. Ce module vous montre comment fonctionne la m√©moire ; plus tard vous apprendrez quand r√©sumer, quand oublier, et quand r√©cup√©rer.

## Prochaines √©tapes

**Module suivant :** [03-rag - RAG (G√©n√©ration augment√©e par r√©cup√©ration)](../03-rag/README.md)

---

**Navigation :** [‚Üê Pr√©c√©dent : Module 01 - Introduction](../01-introduction/README.md) | [Retour au principal](../README.md) | [Suivant : Module 03 - RAG ‚Üí](../03-rag/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Avertissement** :  
Ce document a √©t√© traduit √† l‚Äôaide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d‚Äôassurer l‚Äôexactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d‚Äôorigine doit √™tre consid√©r√© comme la source faisant foi. Pour les informations critiques, une traduction professionnelle r√©alis√©e par un humain est recommand√©e. Nous d√©clinons toute responsabilit√© en cas de malentendus ou de mauvaises interpr√©tations r√©sultant de l‚Äôutilisation de cette traduction.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->