<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c3e07ca58d0b8a3f47d3bf5728541e0a",
  "translation_date": "2025-12-13T13:00:00+00:00",
  "source_file": "01-introduction/README.md",
  "language_code": "fr"
}
-->
# Module 01 : Premiers pas avec LangChain4j

## Table des mati√®res

- [Ce que vous apprendrez](../../../01-introduction)
- [Pr√©requis](../../../01-introduction)
- [Comprendre le probl√®me central](../../../01-introduction)
- [Comprendre les tokens](../../../01-introduction)
- [Comment fonctionne la m√©moire](../../../01-introduction)
- [Comment cela utilise LangChain4j](../../../01-introduction)
- [D√©ployer l'infrastructure Azure OpenAI](../../../01-introduction)
- [Ex√©cuter l'application localement](../../../01-introduction)
- [Utiliser l'application](../../../01-introduction)
  - [Chat sans √©tat (panneau de gauche)](../../../01-introduction)
  - [Chat avec √©tat (panneau de droite)](../../../01-introduction)
- [√âtapes suivantes](../../../01-introduction)

## Ce que vous apprendrez

Si vous avez termin√© le d√©marrage rapide, vous avez vu comment envoyer des invites et obtenir des r√©ponses. C'est la base, mais les applications r√©elles ont besoin de plus. Ce module vous apprend √† construire une IA conversationnelle qui se souvient du contexte et maintient l'√©tat - la diff√©rence entre une d√©mo ponctuelle et une application pr√™te pour la production.

Nous utiliserons GPT-5 d'Azure OpenAI tout au long de ce guide car ses capacit√©s avanc√©es de raisonnement rendent le comportement des diff√©rents mod√®les plus √©vident. Lorsque vous ajoutez la m√©moire, vous verrez clairement la diff√©rence. Cela facilite la compr√©hension de ce que chaque composant apporte √† votre application.

Vous construirez une application qui d√©montre les deux mod√®les :

**Chat sans √©tat** - Chaque requ√™te est ind√©pendante. Le mod√®le n'a aucun souvenir des messages pr√©c√©dents. C'est le mod√®le que vous avez utilis√© dans le d√©marrage rapide.

**Conversation avec √©tat** - Chaque requ√™te inclut l'historique de la conversation. Le mod√®le maintient le contexte sur plusieurs √©changes. C'est ce que les applications en production exigent.

## Pr√©requis

- Abonnement Azure avec acc√®s √† Azure OpenAI
- Java 21, Maven 3.9+
- Azure CLI (https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)
- Azure Developer CLI (azd) (https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd)

> **Note :** Java, Maven, Azure CLI et Azure Developer CLI (azd) sont pr√©install√©s dans le devcontainer fourni.

> **Note :** Ce module utilise GPT-5 sur Azure OpenAI. Le d√©ploiement est configur√© automatiquement via `azd up` - ne modifiez pas le nom du mod√®le dans le code.

## Comprendre le probl√®me central

Les mod√®les de langage sont sans √©tat. Chaque appel API est ind√©pendant. Si vous envoyez "Je m'appelle John" puis demandez "Comment je m'appelle ?", le mod√®le n'a aucune id√©e que vous venez de vous pr√©senter. Il traite chaque requ√™te comme si c'√©tait la premi√®re conversation que vous avez jamais eue.

Cela convient pour des questions-r√©ponses simples mais est inutile pour des applications r√©elles. Les bots de service client doivent se souvenir de ce que vous leur avez dit. Les assistants personnels ont besoin de contexte. Toute conversation √† plusieurs tours n√©cessite une m√©moire.

<img src="../../../translated_images/fr/stateless-vs-stateful.cc4a4765e649c41a.webp" alt="Conversations sans √©tat vs avec √©tat" width="800"/>

*La diff√©rence entre les conversations sans √©tat (appels ind√©pendants) et avec √©tat (conscientes du contexte)*

## Comprendre les tokens

Avant de plonger dans les conversations, il est important de comprendre les tokens - les unit√©s de base de texte que les mod√®les de langage traitent :

<img src="../../../translated_images/fr/token-explanation.c39760d8ec650181.webp" alt="Explication des tokens" width="800"/>

*Exemple de d√©coupage du texte en tokens - "I love AI!" devient 4 unit√©s de traitement distinctes*

Les tokens sont la fa√ßon dont les mod√®les d'IA mesurent et traitent le texte. Les mots, la ponctuation et m√™me les espaces peuvent √™tre des tokens. Votre mod√®le a une limite du nombre de tokens qu'il peut traiter √† la fois (400 000 pour GPT-5, avec jusqu'√† 272 000 tokens en entr√©e et 128 000 tokens en sortie). Comprendre les tokens vous aide √† g√©rer la longueur des conversations et les co√ªts.

## Comment fonctionne la m√©moire

La m√©moire de chat r√©sout le probl√®me sans √©tat en maintenant l'historique de la conversation. Avant d'envoyer votre requ√™te au mod√®le, le framework pr√©fixe les messages pr√©c√©dents pertinents. Quand vous demandez "Comment je m'appelle ?", le syst√®me envoie en fait tout l'historique de la conversation, permettant au mod√®le de voir que vous avez pr√©c√©demment dit "Je m'appelle John."

LangChain4j fournit des impl√©mentations de m√©moire qui g√®rent cela automatiquement. Vous choisissez combien de messages conserver et le framework g√®re la fen√™tre de contexte.

<img src="../../../translated_images/fr/memory-window.bbe67f597eadabb3.webp" alt="Concept de fen√™tre m√©moire" width="800"/>

*MessageWindowChatMemory maintient une fen√™tre glissante des messages r√©cents, supprimant automatiquement les anciens*

## Comment cela utilise LangChain4j

Ce module √©tend le d√©marrage rapide en int√©grant Spring Boot et en ajoutant la m√©moire de conversation. Voici comment les pi√®ces s'assemblent :

**D√©pendances** - Ajoutez deux biblioth√®ques LangChain4j :

```xml
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-open-ai-official</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
```

**Mod√®le de chat** - Configurez Azure OpenAI comme bean Spring ([LangChainConfig.java](../../../01-introduction/src/main/java/com/example/langchain4j/config/LangChainConfig.java)) :

```java
@Bean
public OpenAiOfficialChatModel openAiOfficialChatModel() {
    return OpenAiOfficialChatModel.builder()
            .baseUrl(azureEndpoint)
            .apiKey(azureApiKey)
            .modelName(deploymentName)
            .timeout(Duration.ofMinutes(5))
            .maxRetries(3)
            .build();
}
```

Le builder lit les identifiants depuis les variables d'environnement d√©finies par `azd up`. La d√©finition de `baseUrl` vers votre point de terminaison Azure permet au client OpenAI de fonctionner avec Azure OpenAI.

**M√©moire de conversation** - Suivez l'historique du chat avec MessageWindowChatMemory ([ConversationService.java](../../../01-introduction/src/main/java/com/example/langchain4j/service/ConversationService.java)) :

```java
ChatMemory memory = MessageWindowChatMemory.withMaxMessages(10);

memory.add(UserMessage.from("My name is John"));
memory.add(AiMessage.from("Nice to meet you, John!"));

memory.add(UserMessage.from("What's my name?"));
AiMessage aiMessage = chatModel.chat(memory.messages()).aiMessage();
memory.add(aiMessage);
```

Cr√©ez la m√©moire avec `withMaxMessages(10)` pour conserver les 10 derniers messages. Ajoutez les messages utilisateur et IA avec des wrappers typ√©s : `UserMessage.from(text)` et `AiMessage.from(text)`. R√©cup√©rez l'historique avec `memory.messages()` et envoyez-le au mod√®le. Le service stocke des instances de m√©moire s√©par√©es par ID de conversation, permettant √† plusieurs utilisateurs de discuter simultan√©ment.

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Ouvrez [`ConversationService.java`](../../../01-introduction/src/main/java/com/example/langchain4j/service/ConversationService.java) et demandez :
> - "Comment MessageWindowChatMemory d√©cide-t-il quels messages supprimer lorsque la fen√™tre est pleine ?"
> - "Puis-je impl√©menter un stockage m√©moire personnalis√© utilisant une base de donn√©es au lieu de la m√©moire en RAM ?"
> - "Comment ajouter un r√©sum√© pour compresser l'historique ancien de la conversation ?"

Le point de terminaison de chat sans √©tat ignore compl√®tement la m√©moire - juste `chatModel.chat(prompt)` comme dans le d√©marrage rapide. Le point de terminaison avec √©tat ajoute les messages √† la m√©moire, r√©cup√®re l'historique et inclut ce contexte √† chaque requ√™te. M√™me configuration de mod√®le, mod√®les diff√©rents.

## D√©ployer l'infrastructure Azure OpenAI

**Bash :**
```bash
cd 01-introduction
azd up  # S√©lectionnez l'abonnement et l'emplacement (eastus2 recommand√©)
```

**PowerShell :**
```powershell
cd 01-introduction
azd up  # S√©lectionnez l'abonnement et l'emplacement (eastus2 recommand√©)
```

> **Note :** Si vous rencontrez une erreur de timeout (`RequestConflict: Cannot modify resource ... provisioning state is not terminal`), ex√©cutez simplement `azd up` √† nouveau. Les ressources Azure peuvent encore √™tre en cours de provisionnement en arri√®re-plan, et r√©essayer permet au d√©ploiement de se terminer une fois que les ressources atteignent un √©tat terminal.

Cela va :
1. D√©ployer la ressource Azure OpenAI avec les mod√®les GPT-5 et text-embedding-3-small
2. G√©n√©rer automatiquement le fichier `.env` √† la racine du projet avec les identifiants
3. Configurer toutes les variables d'environnement requises

**Vous avez des probl√®mes de d√©ploiement ?** Consultez le [README Infrastructure](infra/README.md) pour un d√©pannage d√©taill√© incluant les conflits de noms de sous-domaines, les √©tapes de d√©ploiement manuel via le portail Azure, et les conseils de configuration des mod√®les.

**V√©rifiez que le d√©ploiement a r√©ussi :**

**Bash :**
```bash
cat ../.env  # Devrait afficher AZURE_OPENAI_ENDPOINT, API_KEY, etc.
```

**PowerShell :**
```powershell
Get-Content ..\.env  # Devrait afficher AZURE_OPENAI_ENDPOINT, API_KEY, etc.
```

> **Note :** La commande `azd up` g√©n√®re automatiquement le fichier `.env`. Si vous devez le mettre √† jour plus tard, vous pouvez soit √©diter le fichier `.env` manuellement, soit le r√©g√©n√©rer en ex√©cutant :
>
> **Bash :**
> ```bash
> cd ..
> bash .azd-env.sh
> ```
>
> **PowerShell :**
> ```powershell
> cd ..
> .\.azd-env.ps1
> ```

## Ex√©cuter l'application localement

**V√©rifiez le d√©ploiement :**

Assurez-vous que le fichier `.env` existe √† la racine avec les identifiants Azure :

**Bash :**
```bash
cat ../.env  # Devrait afficher AZURE_OPENAI_ENDPOINT, API_KEY, DEPLOYMENT
```

**PowerShell :**
```powershell
Get-Content ..\.env  # Devrait afficher AZURE_OPENAI_ENDPOINT, API_KEY, DEPLOYMENT
```

**D√©marrez les applications :**

**Option 1 : Utiliser le Spring Boot Dashboard (recommand√© pour les utilisateurs VS Code)**

Le conteneur de d√©veloppement inclut l'extension Spring Boot Dashboard, qui fournit une interface visuelle pour g√©rer toutes les applications Spring Boot. Vous pouvez la trouver dans la barre d'activit√© √† gauche de VS Code (cherchez l'ic√¥ne Spring Boot).

Depuis le Spring Boot Dashboard, vous pouvez :
- Voir toutes les applications Spring Boot disponibles dans l'espace de travail
- D√©marrer/arr√™ter les applications en un clic
- Voir les logs des applications en temps r√©el
- Surveiller l'√©tat des applications

Cliquez simplement sur le bouton lecture √† c√¥t√© de "introduction" pour d√©marrer ce module, ou lancez tous les modules en m√™me temps.

<img src="../../../translated_images/fr/dashboard.69c7479aef09ff6b.webp" alt="Spring Boot Dashboard" width="400"/>

**Option 2 : Utiliser les scripts shell**

D√©marrez toutes les applications web (modules 01-04) :

**Bash :**
```bash
cd ..  # Depuis le r√©pertoire racine
./start-all.sh
```

**PowerShell :**
```powershell
cd ..  # Depuis le r√©pertoire racine
.\start-all.ps1
```

Ou d√©marrez uniquement ce module :

**Bash :**
```bash
cd 01-introduction
./start.sh
```

**PowerShell :**
```powershell
cd 01-introduction
.\start.ps1
```

Les deux scripts chargent automatiquement les variables d'environnement depuis le fichier `.env` √† la racine et construiront les JARs s'ils n'existent pas.

> **Note :** Si vous pr√©f√©rez construire manuellement tous les modules avant de d√©marrer :
>
> **Bash :**
> ```bash
> cd ..  # Go to root directory
> mvn clean package -DskipTests
> ```
>
> **PowerShell :**
> ```powershell
> cd ..  # Go to root directory
> mvn clean package -DskipTests
> ```

Ouvrez http://localhost:8080 dans votre navigateur.

**Pour arr√™ter :**

**Bash :**
```bash
./stop.sh  # Ce module uniquement
# Ou
cd .. && ./stop-all.sh  # Tous les modules
```

**PowerShell :**
```powershell
.\stop.ps1  # Ce module uniquement
# Ou
cd ..; .\stop-all.ps1  # Tous les modules
```

## Utiliser l'application

L'application fournit une interface web avec deux impl√©mentations de chat c√¥te √† c√¥te.

<img src="../../../translated_images/fr/home-screen.121a03206ab910c0.webp" alt="√âcran d'accueil de l'application" width="800"/>

*Tableau de bord montrant les options Simple Chat (sans √©tat) et Conversational Chat (avec √©tat)*

### Chat sans √©tat (panneau de gauche)

Essayez d'abord ceci. Dites "Je m'appelle John" puis demandez imm√©diatement "Comment je m'appelle ?" Le mod√®le ne s'en souviendra pas car chaque message est ind√©pendant. Cela d√©montre le probl√®me central de l'int√©gration basique des mod√®les de langage - pas de contexte de conversation.

<img src="../../../translated_images/fr/simple-chat-stateless-demo.13aeb3978eab3234.webp" alt="D√©mo Chat sans √©tat" width="800"/>

*L'IA ne se souvient pas de votre nom du message pr√©c√©dent*

### Chat avec √©tat (panneau de droite)

Essayez maintenant la m√™me s√©quence ici. Dites "Je m'appelle John" puis "Comment je m'appelle ?" Cette fois, il s'en souvient. La diff√©rence est MessageWindowChatMemory - il maintient l'historique de la conversation et l'inclut √† chaque requ√™te. C'est ainsi que fonctionne l'IA conversationnelle en production.

<img src="../../../translated_images/fr/conversational-chat-stateful-demo.e5be9822eb23ff59.webp" alt="D√©mo Chat avec √©tat" width="800"/>

*L'IA se souvient de votre nom plus t√¥t dans la conversation*

Les deux panneaux utilisent le m√™me mod√®le GPT-5. La seule diff√©rence est la m√©moire. Cela montre clairement ce que la m√©moire apporte √† votre application et pourquoi elle est essentielle pour les cas d'usage r√©els.

## √âtapes suivantes

**Module suivant :** [02-prompt-engineering - Ing√©nierie des invites avec GPT-5](../02-prompt-engineering/README.md)

---

**Navigation :** [‚Üê Pr√©c√©dent : Module 00 - D√©marrage rapide](../00-quick-start/README.md) | [Retour au principal](../README.md) | [Suivant : Module 02 - Ing√©nierie des invites ‚Üí](../02-prompt-engineering/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Avertissement** :  
Ce document a √©t√© traduit √† l‚Äôaide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d‚Äôassurer l‚Äôexactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d‚Äôorigine doit √™tre consid√©r√© comme la source faisant foi. Pour les informations critiques, une traduction professionnelle r√©alis√©e par un humain est recommand√©e. Nous d√©clinons toute responsabilit√© en cas de malentendus ou de mauvaises interpr√©tations r√©sultant de l‚Äôutilisation de cette traduction.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->