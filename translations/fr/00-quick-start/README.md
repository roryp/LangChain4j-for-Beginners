<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "377b3e3e6f8d02965bf0fbbc9ccb45c5",
  "translation_date": "2025-12-13T14:27:12+00:00",
  "source_file": "00-quick-start/README.md",
  "language_code": "fr"
}
-->
# Module 00 : D√©marrage rapide

## Table des mati√®res

- [Introduction](../../../00-quick-start)
- [Qu'est-ce que LangChain4j ?](../../../00-quick-start)
- [D√©pendances LangChain4j](../../../00-quick-start)
- [Pr√©requis](../../../00-quick-start)
- [Configuration](../../../00-quick-start)
  - [1. Obtenez votre jeton GitHub](../../../00-quick-start)
  - [2. Configurez votre jeton](../../../00-quick-start)
- [Ex√©cuter les exemples](../../../00-quick-start)
  - [1. Chat basique](../../../00-quick-start)
  - [2. Mod√®les de prompt](../../../00-quick-start)
  - [3. Appel de fonction](../../../00-quick-start)
  - [4. Q&R sur document (RAG)](../../../00-quick-start)
- [Ce que chaque exemple montre](../../../00-quick-start)
- [√âtapes suivantes](../../../00-quick-start)
- [D√©pannage](../../../00-quick-start)

## Introduction

Ce d√©marrage rapide est con√ßu pour vous permettre de commencer √† utiliser LangChain4j aussi rapidement que possible. Il couvre les bases absolues de la cr√©ation d'applications IA avec LangChain4j et les mod√®les GitHub. Dans les modules suivants, vous utiliserez Azure OpenAI avec LangChain4j pour construire des applications plus avanc√©es.

## Qu'est-ce que LangChain4j ?

LangChain4j est une biblioth√®que Java qui simplifie la cr√©ation d'applications aliment√©es par l'IA. Au lieu de g√©rer des clients HTTP et le parsing JSON, vous travaillez avec des API Java propres.

La "cha√Æne" dans LangChain fait r√©f√©rence √† l'encha√Ænement de plusieurs composants - vous pouvez cha√Æner un prompt √† un mod√®le, puis √† un parseur, ou cha√Æner plusieurs appels IA o√π la sortie d'un appel alimente l'entr√©e suivante. Ce d√©marrage rapide se concentre sur les fondamentaux avant d'explorer des cha√Ænes plus complexes.

<img src="../../../translated_images/langchain-concept.ad1fe6cf063515e1.fr.png" alt="Concept d'encha√Ænement LangChain4j" width="800"/>

*Encha√Ænement des composants dans LangChain4j - les blocs de construction se connectent pour cr√©er des flux de travail IA puissants*

Nous utiliserons trois composants principaux :

**ChatLanguageModel** - L'interface pour les interactions avec le mod√®le IA. Appelez `model.chat("prompt")` et obtenez une cha√Æne de r√©ponse. Nous utilisons `OpenAiOfficialChatModel` qui fonctionne avec des points de terminaison compatibles OpenAI comme les mod√®les GitHub.

**AiServices** - Cr√©e des interfaces de service IA typ√©es. D√©finissez des m√©thodes, annotez-les avec `@Tool`, et LangChain4j g√®re l'orchestration. L'IA appelle automatiquement vos m√©thodes Java quand c'est n√©cessaire.

**MessageWindowChatMemory** - Maintient l'historique de la conversation. Sans cela, chaque requ√™te est ind√©pendante. Avec, l'IA se souvient des messages pr√©c√©dents et maintient le contexte sur plusieurs tours.

<img src="../../../translated_images/architecture.eedc993a1c576839.fr.png" alt="Architecture LangChain4j" width="800"/>

*Architecture LangChain4j - composants principaux travaillant ensemble pour alimenter vos applications IA*

## D√©pendances LangChain4j

Ce d√©marrage rapide utilise deux d√©pendances Maven dans le [`pom.xml`](../../../00-quick-start/pom.xml) :

```xml
<!-- Core LangChain4j library -->
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>

<!-- OpenAI integration (works with GitHub Models) -->
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-open-ai-official</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
```

Le module `langchain4j-open-ai-official` fournit la classe `OpenAiOfficialChatModel` qui se connecte aux API compatibles OpenAI. GitHub Models utilise le m√™me format d'API, donc aucun adaptateur sp√©cial n'est n√©cessaire - il suffit de pointer l'URL de base vers `https://models.github.ai/inference`.

## Pr√©requis

**Utilisez-vous le conteneur de d√©veloppement ?** Java et Maven sont d√©j√† install√©s. Vous avez seulement besoin d'un jeton d'acc√®s personnel GitHub.

**D√©veloppement local :**
- Java 21+, Maven 3.9+
- Jeton d'acc√®s personnel GitHub (instructions ci-dessous)

> **Note :** Ce module utilise `gpt-4.1-nano` de GitHub Models. Ne modifiez pas le nom du mod√®le dans le code - il est configur√© pour fonctionner avec les mod√®les disponibles de GitHub.

## Configuration

### 1. Obtenez votre jeton GitHub

1. Allez sur [Param√®tres GitHub ‚Üí Jetons d'acc√®s personnel](https://github.com/settings/personal-access-tokens)
2. Cliquez sur "G√©n√©rer un nouveau jeton"
3. Donnez un nom descriptif (ex. : "D√©mo LangChain4j")
4. D√©finissez une expiration (7 jours recommand√©)
5. Sous "Autorisations du compte", trouvez "Models" et mettez en "Lecture seule"
6. Cliquez sur "G√©n√©rer le jeton"
7. Copiez et sauvegardez votre jeton - vous ne le verrez plus

### 2. Configurez votre jeton

**Option 1 : Utilisation de VS Code (recommand√©)**

Si vous utilisez VS Code, ajoutez votre jeton dans le fichier `.env` √† la racine du projet :

Si le fichier `.env` n'existe pas, copiez `.env.example` en `.env` ou cr√©ez un nouveau fichier `.env` √† la racine du projet.

**Exemple de fichier `.env` :**
```bash
# Dans /workspaces/LangChain4j-for-Beginners/.env
GITHUB_TOKEN=your_token_here
```

Ensuite, vous pouvez simplement faire un clic droit sur n'importe quel fichier de d√©monstration (ex. `BasicChatDemo.java`) dans l'Explorateur et s√©lectionner **"Run Java"** ou utiliser les configurations de lancement depuis le panneau Ex√©cuter et D√©boguer.

**Option 2 : Utilisation du terminal**

D√©finissez le jeton comme variable d'environnement :

**Bash :**
```bash
export GITHUB_TOKEN=your_token_here
```

**PowerShell :**
```powershell
$env:GITHUB_TOKEN=your_token_here
```

## Ex√©cuter les exemples

**Avec VS Code :** Faites un clic droit sur n'importe quel fichier de d√©monstration dans l'Explorateur et s√©lectionnez **"Run Java"**, ou utilisez les configurations de lancement depuis le panneau Ex√©cuter et D√©boguer (assurez-vous d'avoir ajout√© votre jeton dans le fichier `.env` au pr√©alable).

**Avec Maven :** Vous pouvez aussi ex√©cuter depuis la ligne de commande :

### 1. Chat basique

**Bash :**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

**PowerShell :**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

### 2. Mod√®les de prompt

**Bash :**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.PromptEngineeringDemo
```

**PowerShell :**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.PromptEngineeringDemo
```

Montre les prompts zero-shot, few-shot, chain-of-thought et bas√©s sur un r√¥le.

### 3. Appel de fonction

**Bash :**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.ToolIntegrationDemo
```

**PowerShell :**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.ToolIntegrationDemo
```

L'IA appelle automatiquement vos m√©thodes Java quand c'est n√©cessaire.

### 4. Q&R sur document (RAG)

**Bash :**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.SimpleReaderDemo
```

**PowerShell :**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.SimpleReaderDemo
```

Posez des questions sur le contenu de `document.txt`.

## Ce que chaque exemple montre

**Chat basique** - [BasicChatDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/BasicChatDemo.java)

Commencez ici pour voir LangChain4j dans sa forme la plus simple. Vous cr√©erez un `OpenAiOfficialChatModel`, enverrez un prompt avec `.chat()`, et recevrez une r√©ponse. Cela montre les bases : comment initialiser des mod√®les avec des points de terminaison personnalis√©s et des cl√©s API. Une fois ce mod√®le compris, tout le reste s'appuie dessus.

```java
ChatLanguageModel model = OpenAiOfficialChatModel.builder()
    .baseUrl("https://models.github.ai/inference")
    .apiKey(System.getenv("GITHUB_TOKEN"))
    .modelName("gpt-4.1-nano")
    .build();

String response = model.chat("What is LangChain4j?");
System.out.println(response);
```

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Ouvrez [`BasicChatDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/BasicChatDemo.java) et demandez :
> - "Comment passer de GitHub Models √† Azure OpenAI dans ce code ?"
> - "Quels autres param√®tres puis-je configurer dans OpenAiOfficialChatModel.builder() ?"
> - "Comment ajouter des r√©ponses en streaming au lieu d'attendre la r√©ponse compl√®te ?"

**Ing√©nierie des prompts** - [PromptEngineeringDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/PromptEngineeringDemo.java)

Maintenant que vous savez comment parler √† un mod√®le, explorons ce que vous lui dites. Cette d√©mo utilise la m√™me configuration de mod√®le mais montre quatre mod√®les de prompt diff√©rents. Essayez les prompts zero-shot pour des instructions directes, few-shot qui apprennent √† partir d'exemples, chain-of-thought qui r√©v√®lent les √©tapes de raisonnement, et les prompts bas√©s sur un r√¥le qui d√©finissent le contexte. Vous verrez comment le m√™me mod√®le donne des r√©sultats tr√®s diff√©rents selon la fa√ßon dont vous formulez votre demande.

```java
PromptTemplate template = PromptTemplate.from(
    "What's the best time to visit {{destination}} for {{activity}}?"
);

Prompt prompt = template.apply(Map.of(
    "destination", "Paris",
    "activity", "sightseeing"
));

String response = model.chat(prompt.text());
```

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Ouvrez [`PromptEngineeringDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/PromptEngineeringDemo.java) et demandez :
> - "Quelle est la diff√©rence entre zero-shot et few-shot prompting, et quand utiliser chacun ?"
> - "Comment le param√®tre temp√©rature affecte-t-il les r√©ponses du mod√®le ?"
> - "Quelles sont les techniques pour pr√©venir les attaques par injection de prompt en production ?"
> - "Comment cr√©er des objets PromptTemplate r√©utilisables pour des mod√®les courants ?"

**Int√©gration d'outils** - [ToolIntegrationDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/ToolIntegrationDemo.java)

C'est ici que LangChain4j devient puissant. Vous utiliserez `AiServices` pour cr√©er un assistant IA qui peut appeler vos m√©thodes Java. Il suffit d'annoter les m√©thodes avec `@Tool("description")` et LangChain4j g√®re le reste - l'IA d√©cide automatiquement quand utiliser chaque outil selon ce que l'utilisateur demande. Cela d√©montre l'appel de fonction, une technique cl√© pour construire une IA capable d'agir, pas seulement de r√©pondre.

```java
@Tool("Performs addition of two numeric values")
public double add(double a, double b) {
    return a + b;
}

MathAssistant assistant = AiServices.create(MathAssistant.class, model);
String response = assistant.chat("What is 25 plus 17?");
```

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Ouvrez [`ToolIntegrationDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/ToolIntegrationDemo.java) et demandez :
> - "Comment fonctionne l'annotation @Tool et que fait LangChain4j en coulisses ?"
> - "L'IA peut-elle appeler plusieurs outils en s√©quence pour r√©soudre des probl√®mes complexes ?"
> - "Que se passe-t-il si un outil lance une exception - comment g√©rer les erreurs ?"
> - "Comment int√©grer une vraie API au lieu de cet exemple de calculatrice ?"

**Q&R sur document (RAG)** - [SimpleReaderDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/SimpleReaderDemo.java)

Ici, vous verrez les bases du RAG (g√©n√©ration augment√©e par r√©cup√©ration). Au lieu de s'appuyer sur les donn√©es d'entra√Ænement du mod√®le, vous chargez le contenu de [`document.txt`](../../../00-quick-start/document.txt) et l'incluez dans le prompt. L'IA r√©pond en se basant sur votre document, pas sur ses connaissances g√©n√©rales. C'est la premi√®re √©tape pour construire des syst√®mes qui peuvent travailler avec vos propres donn√©es.

```java
Document document = FileSystemDocumentLoader.loadDocument("document.txt");
String content = document.text();

String prompt = "Based on this document: " + content + 
                "\nQuestion: What is the main topic?";
String response = model.chat(prompt);
```

> **Note :** Cette approche simple charge tout le document dans le prompt. Pour les fichiers volumineux (>10Ko), vous d√©passerez les limites de contexte. Le module 03 couvre la d√©coupe en morceaux et la recherche vectorielle pour les syst√®mes RAG en production.

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Ouvrez [`SimpleReaderDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/SimpleReaderDemo.java) et demandez :
> - "Comment le RAG emp√™che-t-il les hallucinations IA compar√© √† l'utilisation des donn√©es d'entra√Ænement du mod√®le ?"
> - "Quelle est la diff√©rence entre cette approche simple et l'utilisation d'embeddings vectoriels pour la r√©cup√©ration ?"
> - "Comment passer √† l'√©chelle pour g√©rer plusieurs documents ou des bases de connaissances plus grandes ?"
> - "Quelles sont les bonnes pratiques pour structurer le prompt afin que l'IA utilise uniquement le contexte fourni ?"

## D√©bogage

Les exemples incluent `.logRequests(true)` et `.logResponses(true)` pour afficher les appels API dans la console. Cela aide √† r√©soudre les erreurs d'authentification, les limites de taux ou les r√©ponses inattendues. Supprimez ces options en production pour r√©duire le bruit des logs.

## √âtapes suivantes

**Module suivant :** [01-introduction - Premiers pas avec LangChain4j et gpt-5 sur Azure](../01-introduction/README.md)

---

**Navigation :** [‚Üê Retour au principal](../README.md) | [Suivant : Module 01 - Introduction ‚Üí](../01-introduction/README.md)

---

## D√©pannage

### Premi√®re compilation Maven

**Probl√®me :** Le premier `mvn clean compile` ou `mvn package` prend beaucoup de temps (10-15 minutes)

**Cause :** Maven doit t√©l√©charger toutes les d√©pendances du projet (Spring Boot, biblioth√®ques LangChain4j, SDK Azure, etc.) lors de la premi√®re compilation.

**Solution :** C'est un comportement normal. Les compilations suivantes seront beaucoup plus rapides car les d√©pendances sont mises en cache localement. Le temps de t√©l√©chargement d√©pend de votre vitesse r√©seau.

### Syntaxe des commandes Maven sous PowerShell

**Probl√®me :** Les commandes Maven √©chouent avec l'erreur `Unknown lifecycle phase ".mainClass=..."`

**Cause :** PowerShell interpr√®te `=` comme un op√©rateur d'affectation de variable, ce qui casse la syntaxe des propri√©t√©s Maven.

**Solution :** Utilisez l'op√©rateur stop-parsing `--%` avant la commande Maven :

**PowerShell :**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

**Bash :**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

L'op√©rateur `--%` indique √† PowerShell de passer tous les arguments restants litt√©ralement √† Maven sans les interpr√©ter.

### Affichage des emojis sous Windows PowerShell

**Probl√®me :** Les r√©ponses IA affichent des caract√®res illisibles (ex. `????` ou `√¢??`) au lieu des emojis dans PowerShell

**Cause :** L'encodage par d√©faut de PowerShell ne supporte pas les emojis UTF-8

**Solution :** Ex√©cutez cette commande avant de lancer les applications Java :
```cmd
chcp 65001
```

Cela force l'encodage UTF-8 dans le terminal. Sinon, utilisez Windows Terminal qui supporte mieux Unicode.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Avertissement** :  
Ce document a √©t√© traduit √† l‚Äôaide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d‚Äôassurer l‚Äôexactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d‚Äôorigine doit √™tre consid√©r√© comme la source faisant foi. Pour les informations critiques, une traduction professionnelle r√©alis√©e par un humain est recommand√©e. Nous d√©clinons toute responsabilit√© en cas de malentendus ou de mauvaises interpr√©tations r√©sultant de l‚Äôutilisation de cette traduction.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->