<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "22b5d7c8d7585325e38b37fd29eafe25",
  "translation_date": "2026-01-05T21:13:24+00:00",
  "source_file": "00-quick-start/README.md",
  "language_code": "fr"
}
-->
# Module 00 : D√©marrage Rapide

## Table des Mati√®res

- [Introduction](../../../00-quick-start)
- [Qu'est-ce que LangChain4j ?](../../../00-quick-start)
- [D√©pendances LangChain4j](../../../00-quick-start)
- [Pr√©requis](../../../00-quick-start)
- [Configuration](../../../00-quick-start)
  - [1. Obtenez votre token GitHub](../../../00-quick-start)
  - [2. Configurez votre token](../../../00-quick-start)
- [Ex√©cuter les Exemples](../../../00-quick-start)
  - [1. Chat basique](../../../00-quick-start)
  - [2. Mod√®les de prompt](../../../00-quick-start)
  - [3. Appel de fonctions](../../../00-quick-start)
  - [4. Q&R sur document (RAG)](../../../00-quick-start)
  - [5. IA Responsable](../../../00-quick-start)
- [Ce que chaque exemple montre](../../../00-quick-start)
- [√âtapes suivantes](../../../00-quick-start)
- [D√©pannage](../../../00-quick-start)

## Introduction

Ce d√©marrage rapide est con√ßu pour vous permettre de commencer √† utiliser LangChain4j le plus rapidement possible. Il couvre les bases absolues de la cr√©ation d'applications IA avec LangChain4j et les mod√®les GitHub. Dans les modules suivants, vous utiliserez Azure OpenAI avec LangChain4j pour construire des applications plus avanc√©es.

## Qu'est-ce que LangChain4j ?

LangChain4j est une biblioth√®que Java qui facilite la cr√©ation d'applications propuls√©es par l'IA. Plut√¥t que de g√©rer des clients HTTP et l'analyse JSON, vous travaillez avec des API Java propres.

La "cha√Æne" dans LangChain fait r√©f√©rence au cha√Ænage de plusieurs composants - vous pouvez cha√Æner un prompt √† un mod√®le puis √† un parseur, ou cha√Æner plusieurs appels IA o√π une sortie alimente la prochaine entr√©e. Ce d√©marrage rapide se concentre sur les fondamentaux avant d'explorer des cha√Ænes plus complexes.

<img src="../../../translated_images/langchain-concept.ad1fe6cf063515e1.fr.png" alt="Concept de Cha√Ænage LangChain4j" width="800"/>

*Cha√Ænage des composants dans LangChain4j - des blocs de construction connect√©s pour cr√©er des workflows IA puissants*

Nous utiliserons trois composants principaux :

**ChatLanguageModel** - L'interface pour les interactions avec le mod√®le IA. Appelez `model.chat("prompt")` et obtenez une r√©ponse sous forme de cha√Æne. Nous utilisons `OpenAiOfficialChatModel` qui fonctionne avec les endpoints compatibles OpenAI comme les mod√®les GitHub.

**AiServices** - Cr√©e des interfaces de services IA typ√©es. D√©finissez des m√©thodes, annotez-les avec `@Tool`, et LangChain4j g√®re l'orchestration. L'IA appelle automatiquement vos m√©thodes Java quand n√©cessaire.

**MessageWindowChatMemory** - Maintient l'historique de la conversation. Sans cela, chaque requ√™te est ind√©pendante. Avec, l'IA se souvient des messages pr√©c√©dents et conserve le contexte sur plusieurs √©changes.

<img src="../../../translated_images/architecture.eedc993a1c576839.fr.png" alt="Architecture LangChain4j" width="800"/>

*Architecture LangChain4j - composants centraux collaborant pour alimenter vos applications IA*

## D√©pendances LangChain4j

Ce d√©marrage rapide utilise deux d√©pendances Maven dans le [`pom.xml`](../../../00-quick-start/pom.xml) :

```xml
<!-- Core LangChain4j library -->
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>

<!-- OpenAI integration (works with GitHub Models) -->
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-open-ai-official</artifactId> <!-- Inherited from BOM in root pom.xml -->
</dependency>
```

Le module `langchain4j-open-ai-official` fournit la classe `OpenAiOfficialChatModel` qui se connecte aux APIs compatibles OpenAI. Les mod√®les GitHub utilisent le m√™me format d‚ÄôAPI, donc aucun adaptateur sp√©cial n‚Äôest n√©cessaire - il suffit de pointer l‚ÄôURL de base vers `https://models.github.ai/inference`.

## Pr√©requis

**Utilisez-vous le conteneur de d√©veloppement ?** Java et Maven sont d√©j√† install√©s. Vous avez seulement besoin d'un jeton d'acc√®s personnel GitHub.

**D√©veloppement local :**
- Java 21+, Maven 3.9+
- Jeton d'acc√®s personnel GitHub (instructions ci-dessous)

> **Note :** Ce module utilise `gpt-4.1-nano` des mod√®les GitHub. Ne modifiez pas le nom du mod√®le dans le code - il est configur√© pour fonctionner avec les mod√®les disponibles sur GitHub.

## Configuration

### 1. Obtenez votre token GitHub

1. Allez sur [Param√®tres GitHub ‚Üí Jetons d‚Äôacc√®s personnel](https://github.com/settings/personal-access-tokens)
2. Cliquez sur ¬´ G√©n√©rer un nouveau jeton ¬ª
3. Mettez un nom descriptif (ex. ¬´ D√©mo LangChain4j ¬ª)
4. D√©finissez une expiration (7 jours recommand√©)
5. Sous ¬´ Permissions du compte ¬ª, trouvez ¬´ Models ¬ª et d√©finissez en ¬´ Lecture seule ¬ª
6. Cliquez sur ¬´ G√©n√©rer le jeton ¬ª
7. Copiez et sauvegardez votre jeton ‚Äì vous ne le verrez plus

### 2. Configurez votre token

**Option 1 : Utiliser VS Code (recommand√©)**

Si vous utilisez VS Code, ajoutez votre token dans le fichier `.env` √† la racine du projet :

Si le fichier `.env` n‚Äôexiste pas, copiez `.env.example` vers `.env` ou cr√©ez un nouveau fichier `.env` √† la racine du projet.

**Exemple de fichier `.env` :**
```bash
# Dans /workspaces/LangChain4j-for-Beginners/.env
GITHUB_TOKEN=your_token_here
```

Vous pouvez ensuite simplement faire un clic droit sur n‚Äôimporte quel fichier de d√©monstration (ex. `BasicChatDemo.java`) dans l‚ÄôExplorateur et s√©lectionner **¬´ Run Java ¬ª** ou utiliser les configurations de lancement dans le panneau Ex√©cuter et D√©boguer.

**Option 2 : Utiliser le terminal**

D√©finissez le token comme variable d‚Äôenvironnement :

**Bash :**
```bash
export GITHUB_TOKEN=your_token_here
```

**PowerShell :**
```powershell
$env:GITHUB_TOKEN=your_token_here
```

## Ex√©cuter les Exemples

**Avec VS Code :** Cliquez droit sur un fichier d√©mo dans l‚ÄôExplorateur et choisissez **¬´ Run Java ¬ª**, ou utilisez les configurations de lancement dans le panneau Ex√©cuter et D√©boguer (assurez-vous d‚Äôavoir ajout√© votre token dans `.env` avant).

**Avec Maven :** Vous pouvez aussi ex√©cuter en ligne de commande :

### 1. Chat basique

**Bash :**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

**PowerShell :**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

### 2. Mod√®les de prompt

**Bash :**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.PromptEngineeringDemo
```

**PowerShell :**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.PromptEngineeringDemo
```

Montre les approches zero-shot, few-shot, cha√Æne de pens√©e, et prompt bas√© sur les r√¥les.

### 3. Appel de fonctions

**Bash :**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.ToolIntegrationDemo
```

**PowerShell :**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.ToolIntegrationDemo
```

L‚ÄôIA appelle automatiquement vos m√©thodes Java quand n√©cessaire.

### 4. Q&R sur document (RAG)

**Bash :**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.SimpleReaderDemo
```

**PowerShell :**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.SimpleReaderDemo
```

Posez des questions sur le contenu de `document.txt`.

### 5. IA Responsable

**Bash :**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.ResponsibleAIDemo
```

**PowerShell :**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.ResponsibleAIDemo
```

Voyez comment les filtres de s√©curit√© IA bloquent les contenus nuisibles.

## Ce que chaque exemple montre

**Chat basique** - [BasicChatDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/BasicChatDemo.java)

Commencez ici pour voir LangChain4j dans sa forme la plus simple. Vous cr√©erez un `OpenAiOfficialChatModel`, enverrez un prompt avec `.chat()`, et recevrez une r√©ponse. Cela montre les bases : comment initialiser les mod√®les avec des endpoints et cl√©s API personnalis√©s. Une fois ce mod√®le compris, tout le reste se construit dessus.

```java
ChatLanguageModel model = OpenAiOfficialChatModel.builder()
    .baseUrl("https://models.github.ai/inference")
    .apiKey(System.getenv("GITHUB_TOKEN"))
    .modelName("gpt-4.1-nano")
    .build();

String response = model.chat("What is LangChain4j?");
System.out.println(response);
```

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Ouvrez [`BasicChatDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/BasicChatDemo.java) et demandez :
> - ¬´ Comment passer de GitHub Models √† Azure OpenAI dans ce code ? ¬ª
> - ¬´ Quels autres param√®tres puis-je configurer dans OpenAiOfficialChatModel.builder() ? ¬ª
> - ¬´ Comment ajouter la diffusion en continu des r√©ponses au lieu d‚Äôattendre la r√©ponse compl√®te ? ¬ª

**Conception de prompt** - [PromptEngineeringDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/PromptEngineeringDemo.java)

Maintenant que vous savez comment parler √† un mod√®le, explorons ce que vous lui dites. Cette d√©mo utilise la m√™me configuration de mod√®le mais montre quatre mod√®les de prompt diff√©rents. Essayez les prompts zero-shot pour des instructions directes, few-shot qui apprennent d‚Äôexemples, cha√Æne de pens√©e qui r√©v√®lent les √©tapes de raisonnement, et prompts bas√©s sur un r√¥le qui d√©finissent le contexte. Vous verrez comment un m√™me mod√®le donne des r√©sultats tr√®s diff√©rents selon la formulation de la demande.

```java
PromptTemplate template = PromptTemplate.from(
    "What's the best time to visit {{destination}} for {{activity}}?"
);

Prompt prompt = template.apply(Map.of(
    "destination", "Paris",
    "activity", "sightseeing"
));

String response = model.chat(prompt.text());
```

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Ouvrez [`PromptEngineeringDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/PromptEngineeringDemo.java) et demandez :
> - ¬´ Quelle est la diff√©rence entre zero-shot et few-shot prompting, et quand utiliser chacun ? ¬ª
> - ¬´ Comment le param√®tre temp√©rature affecte-t-il les r√©ponses du mod√®le ? ¬ª
> - ¬´ Quelles techniques existent pour pr√©venir les attaques par injection de prompt en production ? ¬ª
> - ¬´ Comment cr√©er des objets PromptTemplate r√©utilisables pour des patterns courants ? ¬ª

**Int√©gration d‚Äôoutils** - [ToolIntegrationDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/ToolIntegrationDemo.java)

C‚Äôest ici que LangChain4j devient puissant. Vous utiliserez `AiServices` pour cr√©er un assistant IA capable d‚Äôappeler vos m√©thodes Java. Il suffit d‚Äôannoter les m√©thodes avec `@Tool("description")` et LangChain4j s‚Äôoccupe du reste - l‚ÄôIA d√©cide automatiquement quand utiliser chaque outil selon les requ√™tes utilisateur. Ceci illustre l‚Äôappel de fonctions, une technique cl√© pour b√¢tir une IA capable d‚Äôagir, pas juste de r√©pondre.

```java
@Tool("Performs addition of two numeric values")
public double add(double a, double b) {
    return a + b;
}

MathAssistant assistant = AiServices.create(MathAssistant.class, model);
String response = assistant.chat("What is 25 plus 17?");
```

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Ouvrez [`ToolIntegrationDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/ToolIntegrationDemo.java) et demandez :
> - ¬´ Comment fonctionne l‚Äôannotation @Tool et que fait LangChain4j en coulisses avec celle-ci ? ¬ª
> - ¬´ L‚ÄôIA peut-elle appeler plusieurs outils en s√©quence pour r√©soudre des probl√®mes complexes ? ¬ª
> - ¬´ Que se passe-t-il si un outil g√©n√®re une exception - comment g√©rer les erreurs ? ¬ª
> - ¬´ Comment int√©grer une vraie API √† la place de cet exemple de calculatrice ? ¬ª

**Q&R sur document (RAG)** - [SimpleReaderDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/SimpleReaderDemo.java)

Ici, vous verrez les bases du RAG (retrieval-augmented generation). Au lieu de s‚Äôappuyer sur les donn√©es d‚Äôentra√Ænement du mod√®le, vous chargez le contenu du fichier [`document.txt`](../../../00-quick-start/document.txt) et l‚Äôincluez dans le prompt. L‚ÄôIA r√©pond en se basant sur votre document, pas sur ses connaissances g√©n√©rales. C‚Äôest la premi√®re √©tape pour construire des syst√®mes capables de travailler avec vos propres donn√©es.

```java
Document document = FileSystemDocumentLoader.loadDocument("document.txt");
String content = document.text();

String prompt = "Based on this document: " + content + 
                "\nQuestion: What is the main topic?";
String response = model.chat(prompt);
```

> **Note :** Cette approche simple charge tout le document dans le prompt. Pour de gros fichiers (>10 Ko), vous d√©passerez les limites de contexte. Le module 03 couvre la segmentation et la recherche vectorielle pour les syst√®mes RAG en production.

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Ouvrez [`SimpleReaderDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/SimpleReaderDemo.java) et demandez :
> - ¬´ Comment le RAG emp√™che-t-il les hallucinations IA compar√© √† l‚Äôutilisation des donn√©es d‚Äôentra√Ænement du mod√®le ? ¬ª
> - ¬´ Quelle est la diff√©rence entre cette approche simple et l‚Äôutilisation d‚Äôembeddings vectoriels pour la recherche ? ¬ª
> - ¬´ Comment scaler ceci pour g√©rer plusieurs documents ou bases de connaissances plus larges ? ¬ª
> - ¬´ Quelles sont les bonnes pratiques pour structurer le prompt afin que l‚ÄôIA utilise uniquement le contexte fourni ? ¬ª

**IA Responsable** - [ResponsibleAIDemo.java](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/ResponsibleAIDemo.java)

Construisez la s√©curit√© IA avec une d√©fense en profondeur. Cette d√©mo montre deux couches de protection qui fonctionnent ensemble :

**Partie 1 : LangChain4j Input Guardrails** - Bloque les prompts dangereux avant qu‚Äôils n‚Äôatteignent le LLM. Cr√©ez des garde-fous personnalis√©s qui v√©rifient les mots-cl√©s ou motifs interdits. Ceux-ci s‚Äôex√©cutent dans votre code, donc ils sont rapides et gratuits.

```java
class DangerousContentGuardrail implements InputGuardrail {
    @Override
    public InputGuardrailResult validate(UserMessage userMessage) {
        String text = userMessage.singleText().toLowerCase();
        if (text.contains("explosives")) {
            return fatal("Blocked: contains prohibited keyword");
        }
        return success();
    }
}
```

**Partie 2 : Filtres de s√©curit√© du fournisseur** - Les mod√®les GitHub int√®grent des filtres qui captent ce que vos garde-fous peuvent manquer. Vous verrez des blocages durs (erreurs HTTP 400) pour violations graves et des refus souples o√π l‚ÄôIA d√©cline poliment.

> **ü§ñ Essayez avec [GitHub Copilot](https://github.com/features/copilot) Chat :** Ouvrez [`ResponsibleAIDemo.java`](../../../00-quick-start/src/main/java/com/example/langchain4j/quickstart/ResponsibleAIDemo.java) et demandez :
> - ¬´ Qu‚Äôest-ce que InputGuardrail et comment en cr√©er un personnalis√© ? ¬ª
> - ¬´ Quelle est la diff√©rence entre un blocage dur et un refus souple ? ¬ª
> - ¬´ Pourquoi utiliser √† la fois des garde-fous et des filtres fournisseurs ? ¬ª

## √âtapes suivantes

**Module suivant :** [01-introduction - Premiers pas avec LangChain4j et gpt-5 sur Azure](../01-introduction/README.md)

---

**Navigation :** [‚Üê Retour au principal](../README.md) | [Suivant : Module 01 - Introduction ‚Üí](../01-introduction/README.md)

---

## D√©pannage

### Premi√®re compilation Maven

**Probl√®me :** La commande initiale `mvn clean compile` ou `mvn package` prend beaucoup de temps (10-15 minutes)

**Cause :** Maven doit t√©l√©charger toutes les d√©pendances du projet (Spring Boot, biblioth√®ques LangChain4j, SDK Azure, etc.) lors de la premi√®re compilation.

**Solution :** C‚Äôest un comportement normal. Les compilations suivantes seront beaucoup plus rapides car les d√©pendances sont mises en cache localement. Le temps de t√©l√©chargement d√©pend de la vitesse de votre connexion.

### Syntaxe des commandes Maven sous PowerShell

**Probl√®me :** Les commandes Maven √©chouent avec l‚Äôerreur `Unknown lifecycle phase ".mainClass=..."`

**Cause :** PowerShell interpr√®te `=` comme op√©rateur d‚Äôaffectation de variable, ce qui perturbe la syntaxe des propri√©t√©s Maven.
**Solution** : Utilisez l‚Äôop√©rateur d‚Äôarr√™t d‚Äôinterpr√©tation `--%` avant la commande Maven :

**PowerShell :**
```powershell
mvn --% compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

**Bash :**
```bash
mvn compile exec:java -Dexec.mainClass=com.example.langchain4j.quickstart.BasicChatDemo
```

L‚Äôop√©rateur `--%` indique √† PowerShell de transmettre tous les arguments restants litt√©ralement √† Maven sans les interpr√©ter.

### Affichage des emojis dans Windows PowerShell

**Probl√®me** : Les r√©ponses de l‚ÄôIA affichent des caract√®res illisibles (ex. : `????` ou `√¢??`) au lieu des emojis dans PowerShell

**Cause** : L‚Äôencodage par d√©faut de PowerShell ne prend pas en charge les emojis UTF-8

**Solution** : Ex√©cutez cette commande avant de lancer les applications Java :
```cmd
chcp 65001
```

Cela force l‚Äôencodage UTF-8 dans le terminal. Vous pouvez √©galement utiliser Windows Terminal qui offre un meilleur support Unicode.

### D√©bogage des appels API

**Probl√®me** : Erreurs d‚Äôauthentification, limites de taux ou r√©ponses inattendues du mod√®le d‚ÄôIA

**Solution** : Les exemples incluent `.logRequests(true)` et `.logResponses(true)` pour afficher les appels API dans la console. Cela aide √† r√©soudre les erreurs d‚Äôauthentification, les limites de taux ou les r√©ponses inattendues. Supprimez ces indicateurs en production pour r√©duire le bruit des journaux.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Avertissement** :  
Ce document a √©t√© traduit √† l‚Äôaide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d‚Äôassurer l‚Äôexactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d‚Äôorigine doit √™tre consid√©r√© comme la source officielle. Pour les informations critiques, une traduction professionnelle humaine est recommand√©e. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d‚Äôinterpr√©tations erron√©es r√©sultant de l‚Äôutilisation de cette traduction.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->